{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297c7b43",
   "metadata": {},
   "source": [
    "## Notebook for Constrastive Learning + linear alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda159a",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
    "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
    "#!unzip -o test.zip -d data\n",
    "#!unzip -o train.zip -d data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/Mamiglia/challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582da8c",
   "metadata": {},
   "source": [
    "### Create Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ff8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTranslator(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style translator from text embedding -> image embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim=1024, img_dim=1536, n_heads=8, n_layers=2, dim_feedforward=2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_ln = nn.LayerNorm(text_dim)\n",
    "        self.proj_in = nn.Linear(text_dim, img_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=img_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True  # for (B, Seq, Dim)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.output_ln = nn.LayerNorm(img_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # (B, 1, text_dim)\n",
    "        x = self.input_ln(x)\n",
    "        x = self.proj_in(x)  # project to model dim\n",
    "        out = self.encoder(x)  # Transformer encoder\n",
    "        out = out.squeeze(1)   # remove sequence dim\n",
    "        return self.output_ln(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMLPTranslator(nn.Module):\n",
    "    def __init__(self, text_dim=1024, img_dim=1536, hidden_dim=2048, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 2\n",
    "        self.input_ln = nn.LayerNorm(text_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # first layer: text_dim -> hidden_dim (no residual yet)\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # hidden residual blocks (hidden_dim -> hidden_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for _ in range(num_layers - 2)\n",
    "        ])\n",
    "\n",
    "        # final projection to image space\n",
    "        self.final_proj = nn.Linear(hidden_dim, img_dim)\n",
    "        self.output_ln = nn.LayerNorm(img_dim)\n",
    "\n",
    "        # input residual to output\n",
    "        if text_dim != img_dim:\n",
    "            self.res_proj = nn.Linear(text_dim, img_dim)\n",
    "        else:\n",
    "            self.res_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.input_ln(x)\n",
    "        out = self.first_layer(x_in)\n",
    "        for block in self.blocks:\n",
    "            out = out + block(out)  # residual only between same-dim layers\n",
    "        out = self.final_proj(out)\n",
    "        out = out + self.res_proj(x_in)\n",
    "        return self.output_ln(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSpaceTranslator(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP translator from text embedding -> image embedding\n",
    "    Input: text_emb (batch, text_dim) or (batch, 1, text_dim)\n",
    "    Output: (batch, img_dim)\n",
    "    Regularization: dropout, LayerNorm, GELU, residual (optional projector)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 text_dim=1024,\n",
    "                 img_dim=1536,\n",
    "                 hidden_dim=2048,\n",
    "                 num_layers=3,\n",
    "                 dropout=0.2,\n",
    "                 use_residual=True):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 2, \"num_layers should be >= 2 (including final proj)\"\n",
    "        self.use_residual = use_residual\n",
    "        self.input_ln = nn.LayerNorm(text_dim)\n",
    "        layers = []\n",
    "        in_dim = text_dim\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        # final projection to image space\n",
    "        layers.append(nn.Linear(in_dim, img_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # if using residual, project input to img_dim to add it at the end\n",
    "        if self.use_residual:\n",
    "            if text_dim != img_dim:\n",
    "                self.res_proj = nn.Linear(text_dim, img_dim)\n",
    "            else:\n",
    "                self.res_proj = nn.Identity()\n",
    "\n",
    "        # final layer norm in image space\n",
    "        self.output_ln = nn.LayerNorm(img_dim)\n",
    "\n",
    "    def forward(self, text_emb):\n",
    "        if text_emb.dim() == 3:\n",
    "            x = text_emb.squeeze(1)\n",
    "        else:\n",
    "            x = text_emb\n",
    "        x = self.input_ln(x)\n",
    "        out = self.net(x)  # (B, img_dim)\n",
    "        if self.use_residual:\n",
    "            res = self.res_proj(x)\n",
    "            out = out + res\n",
    "        return self.output_ln(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01a19b",
   "metadata": {},
   "source": [
    "### Training Loop and NCE Loss aswell as Procrustes Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QueueInfoNCELoss(nn.Module):\n",
    "    def __init__(self, dim, temperature=0.07, queue_size=4096):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.queue_size = queue_size\n",
    "        # queue shape: (queue_size, dim)\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, dim))\n",
    "        self.queue = F.normalize(self.queue, dim=1)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _enqueue(self, keys):\n",
    "        \"\"\"\n",
    "        keys: tensor (B, dim), already detached, normalized, on same device as queue.\n",
    "        This writes keys into the circular queue. Safe to call only AFTER backward.\n",
    "        \"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr.item())\n",
    "        end_ptr = (ptr + batch_size) % self.queue_size\n",
    "\n",
    "        if end_ptr > ptr:\n",
    "            self.queue[ptr:end_ptr] = keys\n",
    "        else:\n",
    "            # wrap\n",
    "            first_len = self.queue_size - ptr\n",
    "            self.queue[ptr:] = keys[:first_len]\n",
    "            self.queue[:end_ptr] = keys[first_len:]\n",
    "        self.queue_ptr[0] = end_ptr\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        Computes loss using current queue as negatives but does NOT modify the queue.\n",
    "        z_i: (B, dim) predicted (text -> img)\n",
    "        z_j: (B, dim) target (image)\n",
    "        \"\"\"\n",
    "        # normalize\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # positive logits: (B, 1)\n",
    "        l_pos = torch.sum(z_i * z_j, dim=-1, keepdim=True)\n",
    "\n",
    "        # negative logits from queue: (B, queue_size)\n",
    "        # queue is a buffer; safe to read\n",
    "        l_neg = torch.matmul(z_i, self.queue.T)\n",
    "\n",
    "        # logits: (B, 1 + queue_size)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        logits /= self.temperature\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=z_i.device)  # positives at index 0\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Procrustes initialization ======\n",
    "def procrustes_init(text_embs, img_embs):\n",
    "    \"\"\"\n",
    "    text_embs: (N, d_text)\n",
    "    img_embs:  (N, d_img)\n",
    "    returns: weight matrix (d_img, d_text)\n",
    "    \"\"\"\n",
    "    # Center both\n",
    "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
    "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
    "\n",
    "    # Compute SVD of cross-covariance\n",
    "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
    "    W = U @ Vt  # orthogonal map d_text→d_img\n",
    "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
    "\n",
    "# ====== InfoNCE (CLIP-style) loss ======\n",
    "def info_nce_loss(pred_img_emb, true_img_emb, temperature=0.07):\n",
    "    \"\"\"\n",
    "    pred_img_emb: (B, D)\n",
    "    true_img_emb: (B, D)\n",
    "    \"\"\"\n",
    "    zt = F.normalize(pred_img_emb, dim=1)\n",
    "    zi = F.normalize(true_img_emb, dim=1)\n",
    "    logits = zt @ zi.t() / temperature\n",
    "    labels = torch.arange(len(zt), device=zt.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_i2t + loss_t2i)\n",
    "\n",
    "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
    "    \"\"\"\n",
    "    Apply Procrustes initialization to a model.\n",
    "    - For MLP / ResidualMLP: apply to final Linear layer (hidden -> img_dim)\n",
    "    - For TransformerTranslator: apply to first projection (text_dim -> img_dim)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Compute Procrustes matrix\n",
    "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
    "\n",
    "        # Apply to the appropriate layer\n",
    "        applied = False\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Transformer: apply to first projection (proj_in)\n",
    "                if isinstance(model, TransformerTranslator) and name.endswith(\"proj_in\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "                # MLP / ResidualMLP: apply to final_proj\n",
    "                elif isinstance(model, LatentSpaceTranslator) and name.endswith(\"res_proj\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "\n",
    "                elif isinstance(model, ResidualMLPTranslator) and name.endswith(\"res_proj\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "                    \n",
    "        if not applied:\n",
    "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training loop with Procrustes + InfoNCE ----------\n",
    "def training(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
    "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07,\n",
    "             queue_size=4098):\n",
    "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # --- Optional: Procrustes initialization ---\n",
    "    if use_procrustes_init:\n",
    "        print(\"Computing Procrustes initialization...\")\n",
    "        text_list, img_list = [], []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            text_list.append(X.cpu())\n",
    "            img_list.append(y.cpu())\n",
    "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
    "                break\n",
    "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
    "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
    "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
    "\n",
    "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
    "    # --- Training ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            #loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
    "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
    "            loss += 0.1 * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "              # put them into the queue\n",
    "              criterion._enqueue(keys)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                pred = model(X_batch)\n",
    "                loss = criterion(pred, y_batch)\n",
    "                #loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
    "                loss += 0.1 * F.mse_loss(pred, y_batch)\n",
    "                loss += 0.1 * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "                criterion._enqueue(keys)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        # --- Save best model ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e41a65",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Augmentation\n",
    "# 5. Zero Shot Stitching\n",
    "# 6. Diffusion Priors\n",
    "# Configuration\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 512\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load data\n",
    "train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "DATASET_SIZE = len(X)\n",
    "# Split train/val\n",
    "# This is done only to measure generalization capabilities, you don't have to\n",
    "# use a validation set (though we encourage this)\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bed17",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE):\n",
    "\n",
    "    # --- Common hyperparameters ---\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
    "\n",
    "    # --- New hyperparameters ---\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.01, 0.2)\n",
    "    queue_size = trial.suggest_categorical(\"queue_size\", [2048, 4098, 8196])\n",
    "    #w_infonce = trial.suggest_float(\"w_infonce\", 0.6, 0.8)\n",
    "    #w_cos = trial.suggest_float(\"w_cos\", 0.4, 1.0)\n",
    "    #w_mse = trial.suggest_float(\"w_mse\", 1.0 - w_cos, 1.0)\n",
    "    procrustes_subset = 10000\n",
    "\n",
    "    # --- Architecture-specific hyperparameters ---\n",
    "    if arch in [\"MLP\", \"ResidualMLP\"]:\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [1024, 2048, 4096])\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "        if arch == \"MLP\":\n",
    "            model = LatentSpaceTranslator(\n",
    "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
    "                num_layers=num_layers, dropout=dropout\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = ResidualMLPTranslator(\n",
    "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
    "                num_layers=num_layers, dropout=dropout\n",
    "            ).to(device)\n",
    "    elif arch == \"Transformer\":\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
    "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8, 12])\n",
    "        dim_feedforward = trial.suggest_categorical(\"dim_feedforward\", [1024, 2048, 4096])\n",
    "        model = TransformerTranslator(\n",
    "            text_dim=1024, img_dim=1536,\n",
    "            n_heads=n_heads, n_layers=n_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    # --- Apply Procrustes initialization ---\n",
    "    if procrustes_subset > 0:\n",
    "        # Get subset from train_loader\n",
    "        text_list, img_list = [], []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            text_list.append(X.cpu())\n",
    "            img_list.append(y.cpu())\n",
    "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
    "                break\n",
    "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
    "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
    "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
    "\n",
    "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
    "    # --- Training loop (short run) ---\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(5):  # short training\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "\n",
    "            # Weighted combination of losses\n",
    "            loss = criterion(pred, y_batch)\n",
    "            #loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
    "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
    "            loss += 0.1 * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "              # put them into the queue\n",
    "              criterion._enqueue(keys)\n",
    "\n",
    "    # --- Evaluate on validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            # Use combined loss for evaluation\n",
    "            loss = criterion(pred, y_batch)\n",
    "            #loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
    "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
    "            loss += 0.1 * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            keys = F.normalize(y_batch, dim=1).detach()\n",
    "            criterion._enqueue(keys)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def run_optuna_extended(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=20):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
    "                   n_trials=n_trials)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Val loss: {trial.value}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = ['MLP', 'ResidualMLP', 'Transformer']\n",
    "choosen_arch = archs[2]\n",
    "best_params = run_optuna_extended(\n",
    "    arch = choosen_arch,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    device=DEVICE,\n",
    "    MODEL_PATH_BASE=\"models/translator_optuna\"\n",
    ")\n",
    "\n",
    "if choosen_arch == 'Transformer':\n",
    "    model = TransformerTranslator(\n",
    "        text_dim=1024,\n",
    "        img_dim=1536,\n",
    "        n_heads = best_params['n_heads'],\n",
    "        n_layers=best_params['n_layers'],\n",
    "        dim_feedforward=best_params['dim_feedforward'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data//models/transformer.pth\"\n",
    "\n",
    "elif choosen_arch == 'MLP':\n",
    "    model = LatentSpaceTranslator(\n",
    "    text_dim=1024,\n",
    "    img_dim=1536,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data/models/latent_space.pth\"\n",
    "\n",
    "else:\n",
    "    model = ResidualMLPTranslator(\n",
    "    text_dim=1024,\n",
    "    img_dim=1536,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data/models/residual.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e38bcc",
   "metadata": {},
   "source": [
    "### Training and Submission File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n3. Training...\")\n",
    "model = training(model,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 DEVICE,\n",
    "                 EPOCHS,\n",
    "                 best_params[\"lr\"],\n",
    "                 MODEL_PATH,\n",
    "                 True,\n",
    "                 10000,\n",
    "                 best_params[\"temperature\"],\n",
    "                 best_params[\"queue_size\"])\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff97f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
    "\n",
    "test_embds = test_data['captions/embeddings']\n",
    "test_embds = torch.from_numpy(test_embds).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
    "\n",
    "submission = generate_submission(test_data['captions/ids'], pred_embds, f'{choosen_arch}_submission.csv')\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
