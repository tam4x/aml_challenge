{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryW8Z4mY8Vvo"
      },
      "source": [
        "# **AML Challenge: Model Stitching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQFGDwg7wLL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from PIL import Image\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7zDBTpl9fbd"
      },
      "source": [
        "## **Downloading the Text Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ3Fjmvr8e2c"
      },
      "outputs": [],
      "source": [
        "print(\"🫁 Downloading the model...\")\n",
        "text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBZ2YVHn-Cl8"
      },
      "source": [
        "### **Demo for the Text Encoder:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ9EcikI-Ghn"
      },
      "outputs": [],
      "source": [
        "text = \"A cat is hiding under the table\"\n",
        "\n",
        "# We obtain the \"embedding vector\" using the encode() function:\n",
        "emb = text_encoder.encode(text, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "# This is only for clarity:\n",
        "preview = emb.tolist()[:3] + [\"...\"] + emb.tolist()[-3:]\n",
        "print(f\"The embedding looks like this: {preview}\")\n",
        "\n",
        "# This is the shape of our embedding:\n",
        "print(f\"The shape of the embedding is: {emb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtIPGGaG9jl2"
      },
      "source": [
        "## **Downloading the VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJSK2xPX9bu1"
      },
      "outputs": [],
      "source": [
        "print(\"🫁 Downloading VAE from Hugging Face...\")\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe06iyg8-BXa"
      },
      "source": [
        "### **Demo for the VAE:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg060C8L_fVL"
      },
      "outputs": [],
      "source": [
        "IMG_URL = \"https://images.unsplash.com/photo-1574144611937-0df059b5ef3e?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=764\"\n",
        "img = Image.open(urlopen(IMG_URL)).convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), # --> keep this size fixed\n",
        "    # The VAE works also for 512x512, but it will require more compute\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "# Encode --> latent\n",
        "with torch.no_grad():\n",
        "    latents = vae.encode(img_tensor).latent_dist.sample() * 0.18215\n",
        "print(\"Latent shape:\", latents.shape)\n",
        "\n",
        "# Decode --> reconstruct\n",
        "with torch.no_grad():\n",
        "    recon = vae.decode(latents / 0.18215).sample\n",
        "\n",
        "recon = (recon.clamp(-1, 1) + 1) / 2\n",
        "recon_img = transforms.ToPILImage()(recon.squeeze().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6rxjK5zCDAr"
      },
      "outputs": [],
      "source": [
        "# Visualize input vs output:\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(img.resize((256, 256)))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(recon_img)\n",
        "axes[1].set_title(\"Reconstructed\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FudgLTdoGa5H"
      },
      "source": [
        "#**Frankenstein Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBkYpi5xGhoO"
      },
      "outputs": [],
      "source": [
        "class Translator(nn.Module):\n",
        "    \"\"\"\n",
        "    This will be the Translator Model you have to design for the challenge.\n",
        "    You have (almost) complete freedom on this. Your creativity will be rewarded.\n",
        "\n",
        "    Some ideas might be:\n",
        "    - Zero shot stitching (see https://arxiv.org/pdf/2209.15430)\n",
        "    - Linear, Affine, Orthognal solutions (see https://arxiv.org/pdf/2311.00664)\n",
        "    - Diffusion Priors (see https://arxiv.org/pdf/2204.06125)\n",
        "    - Flow Matching (see https://arxiv.org/pdf/2412.15213)\n",
        "    - CKA / Procrustes Analysis\n",
        "    - Adversarial Trainings\n",
        "    - AutoEncoding Solutions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Here is where *you* come into play:\n",
        "        self.fc = nn.Linear(384, 4 * 32 * 32)\n",
        "        # This is the most trivial thing you can do (spoiler: it doesn't work)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x.view(1, 4, 32, 32)\n",
        "\n",
        "translator = Translator().to(device)\n",
        "translator.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU_2vnkVGuNg"
      },
      "outputs": [],
      "source": [
        "# Part 1: encoding the text prompt\n",
        "text = \"Frankestein's Monster writing code on Google Colab\"\n",
        "print(f\"Prompt: {text}\")\n",
        "emb = text_encoder.encode(text, convert_to_tensor=True).to(device)\n",
        "print(\"Text embedding shape:\", emb.shape)\n",
        "\n",
        "# Part 2: translating the embedding\n",
        "with torch.no_grad():\n",
        "    latent = translator(emb)\n",
        "print(f\"Translated latent shape: {latent.shape}\\n\\n\")\n",
        "\n",
        "# Part 3: feed the VAE with the translation\n",
        "with torch.no_grad():\n",
        "    recon = vae.decode(latent / 0.18215).sample\n",
        "\n",
        "# Part 4: visualizing the output\n",
        "recon = (recon.clamp(-1, 1) + 1) / 2\n",
        "recon_img = transforms.ToPILImage()(recon.squeeze().cpu())\n",
        "\n",
        "plt.imshow(recon_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oi_AfL8tbcw"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XXzzq9AtaIl",
        "outputId": "6fccb966-c876-44db-e0f1-20a1dd5dc5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'challenge'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 98 (delta 39), reused 72 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 21.03 MiB | 13.63 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ],
      "source": [
        "#!mkdir data\n",
        "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "#!unzip -o test.zip -d data\n",
        "#!unzip -o train.zip -d data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kUZM092etlZw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from challenge.src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-style translator from text embedding -> image embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, n_heads=8, n_layers=2, dim_feedforward=2048, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.proj_in = nn.Linear(text_dim, img_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=img_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True  # for (B, Seq, Dim)\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)  # (B, 1, text_dim)\n",
        "        x = self.input_ln(x)\n",
        "        x = self.proj_in(x)  # project to model dim\n",
        "        out = self.encoder(x)  # Transformer encoder\n",
        "        out = out.squeeze(1)   # remove sequence dim\n",
        "        return self.output_ln(out)"
      ],
      "metadata": {
        "id": "VtKo2gJdia3g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualMLPTranslator(nn.Module):\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, hidden_dim=2048, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # first layer: text_dim -> hidden_dim (no residual yet)\n",
        "        self.first_layer = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # hidden residual blocks (hidden_dim -> hidden_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            )\n",
        "            for _ in range(num_layers - 2)\n",
        "        ])\n",
        "\n",
        "        # final projection to image space\n",
        "        self.final_proj = nn.Linear(hidden_dim, img_dim)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "        # input residual to output\n",
        "        if text_dim != img_dim:\n",
        "            self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "        else:\n",
        "            self.res_proj = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = self.input_ln(x)\n",
        "        out = self.first_layer(x_in)\n",
        "        for block in self.blocks:\n",
        "            out = out + block(out)  # residual only between same-dim layers\n",
        "        out = self.final_proj(out)\n",
        "        out = out + self.res_proj(x_in)\n",
        "        return self.output_ln(out)\n"
      ],
      "metadata": {
        "id": "UNbNXAz-7Rg3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentSpaceTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP translator from text embedding -> image embedding\n",
        "    Input: text_emb (batch, text_dim) or (batch, 1, text_dim)\n",
        "    Output: (batch, img_dim)\n",
        "    Regularization: dropout, LayerNorm, GELU, residual (optional projector)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 text_dim=1024,\n",
        "                 img_dim=1536,\n",
        "                 hidden_dim=2048,\n",
        "                 num_layers=3,\n",
        "                 dropout=0.2,\n",
        "                 use_residual=True):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2, \"num_layers should be >= 2 (including final proj)\"\n",
        "        self.use_residual = use_residual\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        layers = []\n",
        "        in_dim = text_dim\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_dim = hidden_dim\n",
        "        # final projection to image space\n",
        "        layers.append(nn.Linear(in_dim, img_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # if using residual, project input to img_dim to add it at the end\n",
        "        if self.use_residual:\n",
        "            if text_dim != img_dim:\n",
        "                self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "            else:\n",
        "                self.res_proj = nn.Identity()\n",
        "\n",
        "        # final layer norm in image space\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        if text_emb.dim() == 3:\n",
        "            x = text_emb.squeeze(1)\n",
        "        else:\n",
        "            x = text_emb\n",
        "        x = self.input_ln(x)\n",
        "        out = self.net(x)  # (B, img_dim)\n",
        "        if self.use_residual:\n",
        "            res = self.res_proj(x)\n",
        "            out = out + res\n",
        "        return self.output_ln(out)\n"
      ],
      "metadata": {
        "id": "TSHxXkcQAKok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training loop with Procrustes + InfoNCE ----------\n",
        "def training(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
        "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07,\n",
        "             w_nce = 0.2, w_mse = 0.2, w_cos = 0.2):\n",
        "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # --- Optional: Procrustes initialization ---\n",
        "    if use_procrustes_init:\n",
        "        print(\"Computing Procrustes initialization...\")\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    # --- Training ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(X_batch)\n",
        "            # Contrastive InfoNCE loss (can mix with cosine or MSE)\n",
        "            loss = w_nce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            # Optional mixed objective:\n",
        "            loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "            loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                pred = model(X_batch)\n",
        "                loss = w_nce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "                # Optional mixed objective:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        # --- Save best model ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3had6TwP7X5E"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Procrustes initialization ======\n",
        "def procrustes_init(text_embs, img_embs):\n",
        "    \"\"\"\n",
        "    text_embs: (N, d_text)\n",
        "    img_embs:  (N, d_img)\n",
        "    returns: weight matrix (d_img, d_text)\n",
        "    \"\"\"\n",
        "    # Center both\n",
        "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
        "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
        "\n",
        "    # Compute SVD of cross-covariance\n",
        "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
        "    W = U @ Vt  # orthogonal map d_text→d_img\n",
        "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
        "\n",
        "# ====== InfoNCE (CLIP-style) loss ======\n",
        "def info_nce_loss(pred_img_emb, true_img_emb, temperature=0.07):\n",
        "    \"\"\"\n",
        "    pred_img_emb: (B, D)\n",
        "    true_img_emb: (B, D)\n",
        "    \"\"\"\n",
        "    zt = F.normalize(pred_img_emb, dim=1)\n",
        "    zi = F.normalize(true_img_emb, dim=1)\n",
        "    logits = zt @ zi.t() / temperature\n",
        "    labels = torch.arange(len(zt), device=zt.device)\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
        "    return 0.5 * (loss_i2t + loss_t2i)\n",
        "\n",
        "def apply_procrustes_init_to_final(model, text_embs, img_embs):\n",
        "    \"\"\"Initialize the final linear layer of model with a Procrustes alignment.\"\"\"\n",
        "    W = procrustes_init(text_embs, img_embs)  # (img_dim, text_dim)\n",
        "    with torch.no_grad():\n",
        "        # find the last Linear layer (hidden_dim -> img_dim)\n",
        "        final_linear = None\n",
        "        for m in reversed(list(model.modules())):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                final_linear = m\n",
        "                break\n",
        "        if final_linear is not None:\n",
        "            # Resize if hidden_dim != text_dim\n",
        "            rows, cols = final_linear.weight.shape\n",
        "            # compress or pad W.T accordingly\n",
        "            if cols != W.shape[1]:\n",
        "                # project W to match\n",
        "                U, S, Vt = torch.linalg.svd(W, full_matrices=False)\n",
        "                W_approx = (U[:, :cols] @ Vt[:cols, :])  # (img_dim, cols)\n",
        "            else:\n",
        "                W_approx = W\n",
        "\n",
        "            # If shape still doesn't match, slice\n",
        "            W_approx = W_approx[:rows, :cols]\n",
        "            final_linear.weight.copy_(W_approx)\n",
        "            print(f\"✓ Applied Procrustes init to final layer: {rows}×{cols}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "gH43_GIXb7xg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHgZaxy_twIO",
        "outputId": "9bd2ff8a-7fd2-4d21-b585-792e39ac7564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000,)\n",
            "Train data: 125000 captions, 125000 images\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([112500, 1536]), torch.Size([112500, 1024]), 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# 1. Learning with a basic simple transformer\n",
        "# 2. Learning with a basic deep neural network\n",
        "# 3. TODO: Creat a hyperparameter optimization for the transformer aswell as\n",
        "# the lost function nce loss etc. and perform it on the first layer of the transformer\n",
        "# Configuration\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load data\n",
        "train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "hls7zZCTleL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "7s541MhBomzy",
        "outputId": "ed01b2e7-35fc-482b-e288-740801e69d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE):\n",
        "    # --- Sample hyperparameters ---\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [1024, 2048, 4096])\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "\n",
        "    # --- New hyperparameters ---\n",
        "    temperature = trial.suggest_float(\"temperature\", 0.01, 0.2)\n",
        "    w_infonce = trial.suggest_float(\"w_infonce\", 0.6, 0.8)\n",
        "    w_cos = trial.suggest_float(\"w_cos\", 0.4, 1.0)\n",
        "    w_mse = trial.suggest_float(\"w_mse\", 1.0 - w_cos, 1.0)\n",
        "    procrustes_subset = 10000\n",
        "\n",
        "    # --- Create model ---\n",
        "\n",
        "    if arch == 'transformer':\n",
        "      model = TransformerTranslator().to(DEVICE)\n",
        "\n",
        "    elif arch == 'nn':\n",
        "        model = LatentSpaceTranslator(\n",
        "        text_dim=1024,\n",
        "        img_dim=1536,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout).to(device)\n",
        "\n",
        "    else:\n",
        "        model = ResidualMLPTranslator(\n",
        "        text_dim=1024,\n",
        "        img_dim=1536,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout).to(DEVICE)\n",
        "\n",
        "\n",
        "    # --- Apply Procrustes initialization ---\n",
        "    if procrustes_subset > 0:\n",
        "        # Get subset from train_loader\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    # --- Training loop (short run) ---\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(5):  # short training\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "            pred = F.normalize(pred, dim=-1)\n",
        "            y_batch = F.normalize(y_batch, dim=-1)\n",
        "\n",
        "            # Weighted combination of losses\n",
        "            loss = 0\n",
        "            if w_infonce > 0:\n",
        "                loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            if w_mse > 0:\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            if w_cos > 0:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- Evaluate on validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            pred = F.normalize(pred, dim=-1)\n",
        "            y_batch = F.normalize(y_batch, dim=-1)\n",
        "            # Use combined loss for evaluation\n",
        "            loss = 0\n",
        "            if w_infonce > 0:\n",
        "                loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            if w_mse > 0:\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            if w_cos > 0:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def run_optuna_extended(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=10):\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
        "                   n_trials=n_trials)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"Val loss: {trial.value}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return trial.params"
      ],
      "metadata": {
        "id": "QUlWlaSRldVd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arch = 'res'\n",
        "best_params = run_optuna_extended(\n",
        "    arch = 'res',\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    device=DEVICE,\n",
        "    MODEL_PATH_BASE=\"models/translator_optuna\"\n",
        ")\n",
        "\n",
        "if arch == 'transformer':\n",
        "    model = TransformerTranslator().to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data//models/transformer.pth\"\n",
        "\n",
        "elif arch == 'nn':\n",
        "    model = LatentSpaceTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/latent_space.pth\"\n",
        "\n",
        "else:\n",
        "    model = ResidualMLPTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/residual.pth\""
      ],
      "metadata": {
        "id": "ciC6HVDky2c1",
        "outputId": "20e0be42-43ef-4cad-c81f-b896eb736d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 11:33:44,983] A new study created in memory with name: no-name-91d0b3c8-efa9-42b8-88e5-131817155c7b\n",
            "/tmp/ipython-input-916805747.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
            "/tmp/ipython-input-916805747.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Applied Procrustes init to final layer: 1536×1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 11:34:08,362] Trial 0 finished with value: 3.3108338871780707 and parameters: {'hidden_dim': 2048, 'num_layers': 3, 'dropout': 0.36071903056866395, 'weight_decay': 0.00020792509380118625, 'lr': 1.2651928325672493e-05, 'batch_size': 64, 'temperature': 0.19896949396211575, 'w_infonce': 0.7007854220900475, 'w_cos': 0.6747202940848545, 'w_mse': 0.5432330993195925}. Best is trial 0 with value: 3.3108338871780707.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Applied Procrustes init to final layer: 1536×1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 11:34:28,433] Trial 1 finished with value: 2.5476222086925895 and parameters: {'hidden_dim': 1024, 'num_layers': 5, 'dropout': 0.24034302193434762, 'weight_decay': 4.217153743745833e-05, 'lr': 1.347217160770757e-05, 'batch_size': 64, 'temperature': 0.11376524249411407, 'w_infonce': 0.6119678078943391, 'w_cos': 0.7392104202237512, 'w_mse': 0.45028797776702034}. Best is trial 1 with value: 2.5476222086925895.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Applied Procrustes init to final layer: 1536×1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-10-31 11:34:30,658] Trial 2 failed with parameters: {'hidden_dim': 2048, 'num_layers': 6, 'dropout': 0.24365242711057036, 'weight_decay': 0.00013847890031289011, 'lr': 0.0008006774692787673, 'batch_size': 256, 'temperature': 0.17450242429350432, 'w_infonce': 0.7763051268812414, 'w_cos': 0.56025858887087, 'w_mse': 0.9779758918753335} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-916805747.py\", line 102, in <lambda>\n",
            "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-916805747.py\", line 68, in objective_extended\n",
            "    loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3241398790.py\", line 24, in info_nce_loss\n",
            "    zi = F.normalize(true_img_emb, dim=1)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 5567, in normalize\n",
            "    denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-10-31 11:34:30,660] Trial 2 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2300375263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'res'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m best_params = run_optuna_extended(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'res'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-916805747.py\u001b[0m in \u001b[0;36mrun_optuna_extended\u001b[0;34m(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_optuna_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH_BASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n\u001b[0m\u001b[1;32m    103\u001b[0m                    n_trials=n_trials)\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-916805747.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_optuna_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH_BASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n\u001b[0m\u001b[1;32m    103\u001b[0m                    n_trials=n_trials)\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-916805747.py\u001b[0m in \u001b[0;36mobjective_extended\u001b[0;34m(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw_infonce\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_infonce\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minfo_nce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw_mse\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_mse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3241398790.py\u001b[0m in \u001b[0;36minfo_nce_loss\u001b[0;34m(pred_img_emb, true_img_emb, temperature)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m     \u001b[0mzt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_img_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mzi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_img_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzt\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mzi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   5565\u001b[0m         )\n\u001b[1;32m   5566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5567\u001b[0;31m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "lE5CwJCFtzfB",
        "outputId": "3a5631d9-62cb-42c3-fe26-3f82fcbdce14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Parameters: 6,830,080\n",
            "\n",
            "3. Training...\n",
            "Computing Procrustes initialization...\n",
            "✓ Applied Procrustes init to final layer: 1536×1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 440/440 [00:04<00:00, 101.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 1.739060, Val Loss = 2.079712\n",
            "  ✓ Saved best model (val_loss=2.079712)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 440/440 [00:04<00:00, 93.82it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 1.404553, Val Loss = 2.005234\n",
            "  ✓ Saved best model (val_loss=2.005234)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 440/440 [00:05<00:00, 83.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 1.290484, Val Loss = 1.960581\n",
            "  ✓ Saved best model (val_loss=1.960581)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 440/440 [00:08<00:00, 54.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 1.202754, Val Loss = 1.926914\n",
            "  ✓ Saved best model (val_loss=1.926914)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 440/440 [00:04<00:00, 98.47it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 1.126508, Val Loss = 1.899866\n",
            "  ✓ Saved best model (val_loss=1.899866)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30:  80%|███████▉  | 351/440 [00:03<00:00, 111.00it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3704954777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n3. Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = training(model, \n\u001b[0m\u001b[1;32m      6\u001b[0m                  \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                  \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-448608328.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH, use_procrustes_init, procrustes_subset, temperature, w_nce, w_mse, w_cos)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_cos\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_mse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train\n",
        "print(\"\\n3. Training...\")\n",
        "model = training(model,\n",
        "                 train_loader,\n",
        "                 val_loader,\n",
        "                 DEVICE,\n",
        "                 EPOCHS,\n",
        "                 best_params[\"lr\"],\n",
        "                 MODEL_PATH,\n",
        "                 True,\n",
        "                 10000,\n",
        "                 best_params[\"temperature\"],\n",
        "                 best_params[\"w_infonce\"],\n",
        "                 best_params[\"w_mse\"],\n",
        "                 best_params[\"w_cos\"])\n",
        "\n",
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "o6uvjZJZt2PI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bafbb52a-b8c9-42a4-e392-ef69d9cd1547"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-651285137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_caption_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'captions/text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTRAIN_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_text_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg_VAL_SPLIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTRAIN_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_img_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images/names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_VAL_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mval_img_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images/embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_VAL_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from challenge.src.eval import visualize_retrieval\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
        "val_text_embd = X_val\n",
        "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
        "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
        "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
        "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:,img_VAL_SPLIT])[1]\n",
        "\n",
        "# Sample and visualize\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(0, 100)\n",
        "    caption_embd = val_text_embd[idx]\n",
        "    caption_text = val_caption_text[idx]\n",
        "    gt_index = val_label[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_embds = model(caption_embd.to(DEVICE)).cpu()\n",
        "\n",
        "        visualize_retrieval(\n",
        "            pred_embds,\n",
        "            gt_index,\n",
        "            val_img_file,\n",
        "            caption_text, val_img_embd, k=5,\n",
        "            dataset_path = 'drive/MyDrive/data/train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RkbJt1-AuCGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db9ab5a-6cca-468e-a68f-dcd419412097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission file...\n",
            "✓ Saved submission to submission.csv\n",
            "Model saved to: drive/MyDrive/data/models/latent_space.pth\n"
          ]
        }
      ],
      "source": [
        "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], pred_embds, 'submission.csv')\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7EYrdanW7Fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}