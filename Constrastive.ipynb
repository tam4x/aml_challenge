{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "297c7b43",
      "metadata": {
        "id": "297c7b43"
      },
      "source": [
        "## Notebook for Constrastive Learning + linear alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eda159a",
      "metadata": {
        "id": "7eda159a"
      },
      "source": [
        "### Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45f4af51",
      "metadata": {
        "id": "45f4af51",
        "outputId": "089e7d97-4b26-40ae-b8a2-1825d6c64141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'challenge'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 98 (delta 39), reused 72 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 21.03 MiB | 20.45 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "--2025-11-12 19:46:57--  https://raw.githubusercontent.com/tam4x/aml_challenge/refs/heads/main/preprocess_data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8066 (7.9K) [text/plain]\n",
            "Saving to: ‘preprocess_data.py’\n",
            "\n",
            "preprocess_data.py  100%[===================>]   7.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-12 19:46:57 (117 MB/s) - ‘preprocess_data.py’ saved [8066/8066]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!mkdir data\n",
        "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "#!unzip -o test.zip -d data\n",
        "#!unzip -o train.zip -d data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/Mamiglia/challenge.git\n",
        "!wget https://raw.githubusercontent.com/tam4x/aml_challenge/refs/heads/main/preprocess_data.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8140ee3f",
      "metadata": {
        "id": "8140ee3f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from challenge.src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from PIL import Image\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "import random\n",
        "# roberta-large-nli-stsb-mean-tokens\n",
        "def load_text_model(model_name=\"sentence-transformers/roberta-large-nli-stsb-mean-tokens\"):\n",
        "    \"\"\"Load Sentence-BERT text encoder.\"\"\"\n",
        "    print(f\"Loading text model: {model_name}\")\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "def load_image_model(model_name=\"facebook/dinov2-giant\"):\n",
        "    \"\"\"Load DINOv2 image encoder.\"\"\"\n",
        "    print(f\"Loading image model: {model_name}\")\n",
        "    image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    return image_processor, model\n",
        "\n",
        "## Can be modified for data augmentation if needed or create my one\n",
        "def get_augmentations(num_augmentations=3):\n",
        "    \"\"\"Create a list of augmentation pipelines (light + heavy mix).\"\"\"\n",
        "    light = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    ])\n",
        "\n",
        "    heavy = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    ])\n",
        "\n",
        "    augs = []\n",
        "    for _ in range(num_augmentations):\n",
        "        augs.append(random.choice([light, heavy]))\n",
        "    return augs\n",
        "\n",
        "@torch.inference_mode()\n",
        "def process_images_batch(image_processor, model, image_paths, device, batch_size=128, dataset_path=None, augmentations=3):\n",
        "    \"\"\"Generate image embeddings in batches.\"\"\"\n",
        "    print(f\"Processing {len(image_paths)} images in batches...\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "    all_names = []\n",
        "    img_files = []\n",
        "    augs = get_augmentations(augmentations)\n",
        "    all_images = []\n",
        "\n",
        "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Encoding images\"):\n",
        "        batch_paths = image_paths[i:i+batch_size]\n",
        "        valid_images = []\n",
        "\n",
        "        # Keep track of which original indices correspond to valid images in the batch\n",
        "        valid_paths = []\n",
        "\n",
        "        for j, path in enumerate(batch_paths):\n",
        "            original_index = i + j\n",
        "            try:\n",
        "                img = Image.open(dataset_path / 'Images' / path).convert(\"RGB\")\n",
        "                valid_images.append(img)\n",
        "                valid_paths.append(path)\n",
        "                for k, aug in enumerate(augs):\n",
        "                  aug_img = aug(img)\n",
        "                  valid_images.append(aug_img)\n",
        "                  valid_paths.append(f\"{path}_aug{k+1}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Skipping image {path} due to error: {e}\")\n",
        "\n",
        "        if not valid_images:\n",
        "            continue\n",
        "\n",
        "        inputs = image_processor(images=valid_images, return_tensors=\"pt\").to(device)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Average over patch tokens\n",
        "        image_features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store embeddings based on their success\n",
        "        all_embeddings.extend(image_features)\n",
        "        img_files.extend(valid_paths)\n",
        "\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "    return all_names, all_embeddings\n",
        "\n",
        "\n",
        "def process_captions(text_model, captions, device):\n",
        "    \"\"\"Generate text embeddings using Sentence-BERT.\"\"\"\n",
        "    print(\"Processing captions...\")\n",
        "    return text_model.encode(\n",
        "        captions,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    \"\"\"\n",
        "    Load dataset from a directory containing captions.txt and an Images folder.\n",
        "    \"\"\"\n",
        "    captions_file = dataset_path / \"captions.txt\"\n",
        "    images_dir = dataset_path / \"Images\"\n",
        "    if not captions_file.exists() or not images_dir.is_dir():\n",
        "        raise FileNotFoundError(f\"Could not find 'captions.txt' or 'Images' directory in {dataset_path}\")\n",
        "\n",
        "    df = pd.read_csv(captions_file)\n",
        "\n",
        "    if 'id' not in df.columns:\n",
        "        df['id'] = np.arange(len(df))\n",
        "\n",
        "    return df\n",
        "\n",
        "num_augmentations = 4\n",
        "dataset_path = Path('drive/MyDrive/data/train/')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "output_file = 'drive/MyDrive/data/augmented_testdata.npz'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(f\"Loading dataset from: {dataset_path}\")\n",
        "df_captions = load_dataset(dataset_path)\n",
        "\n",
        "text_model = load_text_model()\n",
        "image_processor, image_model = load_image_model()\n",
        "\n",
        "all_captions = df_captions['caption'].tolist()\n",
        "caption2img = df_captions['image'].tolist()\n",
        "all_images = df_captions['image'].unique().tolist()\n",
        "\n",
        "num_images = len(all_images)\n",
        "num_captions = len(all_captions)\n",
        "print(f\"Found {num_images} images and {num_captions} total captions.\")\n",
        "\n",
        "all_images, img_embd = process_images_batch(image_processor,\n",
        "                                            image_model,\n",
        "                                            all_images,\n",
        "                                            device,\n",
        "                                            dataset_path=dataset_path, augmentations=num_augmentations)\n",
        "\n",
        "images_dict = {img_name: i for i, img_name in enumerate(all_images)}\n"
      ],
      "metadata": {
        "id": "4NlYDhMIm8ht",
        "outputId": "a23bbc45-9c63-42e1-c25f-b3acceb932f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4NlYDhMIm8ht",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading dataset from: drive/MyDrive/data/train\n",
            "Loading text model: sentence-transformers/roberta-large-nli-stsb-mean-tokens\n",
            "Loading image model: facebook/dinov2-giant\n",
            "Found 25000 images and 125000 total captions.\n",
            "Processing 128 images in batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEncoding images:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate captions per image *and* per augmentation\n",
        "repeated_captions = []\n",
        "repeated_imgnames = []\n",
        "num_captions_per_image = 5\n",
        "num_versions_per_image = 1 + num_augmentations  # original + 3 augmentations\n",
        "\n",
        "for i in range(0, len(all_images), num_versions_per_image):\n",
        "    image_group = all_images[i : i + num_versions_per_image]\n",
        "    for j in range(num_captions_per_image):\n",
        "        caption = all_captions[(i // num_versions_per_image) * num_captions_per_image + j]\n",
        "        for img_variant in image_group:\n",
        "            repeated_captions.append(caption)\n",
        "            repeated_imgnames.append(img_variant)\n",
        "\n",
        "\n",
        "print(len(repeated_captions), len((repeated_imgnames)))\n",
        "caption_embeddings = process_captions(text_model, repeated_captions, device)\n",
        "\n",
        "num_images = len(all_images)\n",
        "num_captions = len(repeated_captions)\n",
        "\n",
        "label = np.zeros((num_captions, num_images), dtype=bool)\n",
        "image_idx_map = {name: i for i, name in enumerate(all_images)}\n",
        "for idx, img_name in enumerate(repeated_imgnames):\n",
        "    if img_name in image_idx_map:\n",
        "        label[idx, image_idx_map[img_name]] = 1\n",
        "\n",
        "data = {\n",
        "    'metadata/num_captions': np.array([num_captions]),\n",
        "    'metadata/num_images': np.array([num_images]),\n",
        "    'metadata/embedding_dim_text': np.array([caption_embeddings.shape[1]]),\n",
        "    'metadata/embedding_dim_image': np.array([img_embd.shape[1]]),\n",
        "    'captions/text': np.array(repeated_captions),\n",
        "    'captions/embeddings': caption_embeddings,\n",
        "    'captions/label': label,\n",
        "    'images/names': np.array(all_images),\n",
        "    'images/embeddings': img_embd,\n",
        "}\n",
        "\n",
        "print(f\"Saving augmented dataset to {output_file}\")\n",
        "np.savez_compressed(output_file, **data)\n",
        "print(\"✓ Done.\")"
      ],
      "metadata": {
        "id": "D9G7nS-tn4HG"
      },
      "id": "D9G7nS-tn4HG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e582da8c",
      "metadata": {
        "id": "e582da8c"
      },
      "source": [
        "### Create Neural Network Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335ff8ba",
      "metadata": {
        "id": "335ff8ba"
      },
      "outputs": [],
      "source": [
        "class TransformerTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-style translator from text embedding -> image embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, n_heads=8, n_layers=2, dim_feedforward=2048, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.proj_in = nn.Linear(text_dim, img_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=img_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True  # for (B, Seq, Dim)\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)  # (B, 1, text_dim)\n",
        "        x = self.input_ln(x)\n",
        "        x = self.proj_in(x)  # project to model dim\n",
        "        out = self.encoder(x)  # Transformer encoder\n",
        "        out = out.squeeze(1)   # remove sequence dim\n",
        "        return self.output_ln(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42f54c4",
      "metadata": {
        "id": "e42f54c4"
      },
      "outputs": [],
      "source": [
        "class ResidualMLPTranslator(nn.Module):\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, hidden_dim=2048, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # first layer: text_dim -> hidden_dim (no residual yet)\n",
        "        self.first_layer = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # hidden residual blocks (hidden_dim -> hidden_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            )\n",
        "            for _ in range(num_layers - 2)\n",
        "        ])\n",
        "\n",
        "        # final projection to image space\n",
        "        self.final_proj = nn.Linear(hidden_dim, img_dim)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "        # input residual to output\n",
        "        if text_dim != img_dim:\n",
        "            self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "        else:\n",
        "            self.res_proj = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = self.input_ln(x)\n",
        "        out = self.first_layer(x_in)\n",
        "        for block in self.blocks:\n",
        "            out = out + block(out)  # residual only between same-dim layers\n",
        "        out = self.final_proj(out)\n",
        "        out = out + self.res_proj(x_in)\n",
        "        return self.output_ln(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f345ba96",
      "metadata": {
        "id": "f345ba96"
      },
      "outputs": [],
      "source": [
        "class LatentSpaceTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP translator from text embedding -> image embedding\n",
        "    Input: text_emb (batch, text_dim) or (batch, 1, text_dim)\n",
        "    Output: (batch, img_dim)\n",
        "    Regularization: dropout, LayerNorm, GELU, residual (optional projector)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 text_dim=1024,\n",
        "                 img_dim=1536,\n",
        "                 hidden_dim=2048,\n",
        "                 num_layers=3,\n",
        "                 dropout=0.2,\n",
        "                 use_residual=True):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2, \"num_layers should be >= 2 (including final proj)\"\n",
        "        self.use_residual = use_residual\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        layers = []\n",
        "        in_dim = text_dim\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_dim = hidden_dim\n",
        "        # final projection to image space\n",
        "        layers.append(nn.Linear(in_dim, img_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # if using residual, project input to img_dim to add it at the end\n",
        "        if self.use_residual:\n",
        "            if text_dim != img_dim:\n",
        "                self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "            else:\n",
        "                self.res_proj = nn.Identity()\n",
        "\n",
        "        # final layer norm in image space\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        if text_emb.dim() == 3:\n",
        "            x = text_emb.squeeze(1)\n",
        "        else:\n",
        "            x = text_emb\n",
        "        x = self.input_ln(x)\n",
        "        out = self.net(x)  # (B, img_dim)\n",
        "        if self.use_residual:\n",
        "            res = self.res_proj(x)\n",
        "            out = out + res\n",
        "        return self.output_ln(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed01a19b",
      "metadata": {
        "id": "ed01a19b"
      },
      "source": [
        "### Training Loop and NCE Loss aswell as Procrustes Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e729786d",
      "metadata": {
        "id": "e729786d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QueueInfoNCELoss(nn.Module):\n",
        "    def __init__(self, dim, temperature=0.07, queue_size=4096):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.queue_size = queue_size\n",
        "        # queue shape: (queue_size, dim)\n",
        "        self.register_buffer(\"queue\", torch.randn(queue_size, dim))\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _enqueue(self, keys):\n",
        "        \"\"\"\n",
        "        keys: tensor (B, dim), already detached, normalized, on same device as queue.\n",
        "        This writes keys into the circular queue. Safe to call only AFTER backward.\n",
        "        \"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "        ptr = int(self.queue_ptr.item())\n",
        "        end_ptr = (ptr + batch_size) % self.queue_size\n",
        "\n",
        "        if end_ptr > ptr:\n",
        "            self.queue[ptr:end_ptr] = keys\n",
        "        else:\n",
        "            # wrap\n",
        "            first_len = self.queue_size - ptr\n",
        "            self.queue[ptr:] = keys[:first_len]\n",
        "            self.queue[:end_ptr] = keys[first_len:]\n",
        "        self.queue_ptr[0] = end_ptr\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        \"\"\"\n",
        "        Computes loss using current queue as negatives but does NOT modify the queue.\n",
        "        z_i: (B, dim) predicted (text -> img)\n",
        "        z_j: (B, dim) target (image)\n",
        "        \"\"\"\n",
        "        # normalize\n",
        "        z_i = F.normalize(z_i, dim=1)\n",
        "        z_j = F.normalize(z_j, dim=1)\n",
        "\n",
        "        # positive logits: (B, 1)\n",
        "        l_pos = torch.sum(z_i * z_j, dim=-1, keepdim=True)\n",
        "\n",
        "        # negative logits from queue: (B, queue_size)\n",
        "        # queue is a buffer; safe to read\n",
        "        l_neg = torch.matmul(z_i, self.queue.T)\n",
        "\n",
        "        # logits: (B, 1 + queue_size)\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "        logits /= self.temperature\n",
        "\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=z_i.device)  # positives at index 0\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f1f386",
      "metadata": {
        "id": "82f1f386"
      },
      "outputs": [],
      "source": [
        "# ====== Procrustes initialization ======\n",
        "def procrustes_init(text_embs, img_embs):\n",
        "    \"\"\"\n",
        "    text_embs: (N, d_text)\n",
        "    img_embs:  (N, d_img)\n",
        "    returns: weight matrix (d_img, d_text)\n",
        "    \"\"\"\n",
        "    # Center both\n",
        "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
        "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
        "\n",
        "    # Compute SVD of cross-covariance\n",
        "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
        "    W = U @ Vt  # orthogonal map d_text→d_img\n",
        "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
        "\n",
        "\n",
        "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
        "    \"\"\"\n",
        "    Apply Procrustes initialization to a model.\n",
        "    - For MLP / ResidualMLP: apply to final Linear layer (hidden -> img_dim)\n",
        "    - For TransformerTranslator: apply to first projection (text_dim -> img_dim)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Compute Procrustes matrix\n",
        "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
        "\n",
        "        # Apply to the appropriate layer\n",
        "        applied = False\n",
        "        for name, m in model.named_modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Transformer: apply to first projection (proj_in)\n",
        "                if isinstance(model, TransformerTranslator) and name.endswith(\"proj_in\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "                # MLP / ResidualMLP: apply to final_proj\n",
        "                elif isinstance(model, LatentSpaceTranslator) and name.endswith(\"res_proj\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "\n",
        "                elif isinstance(model, ResidualMLPTranslator) and name.endswith(\"res_proj\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "\n",
        "        if not applied:\n",
        "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b2b396",
      "metadata": {
        "id": "e7b2b396"
      },
      "outputs": [],
      "source": [
        "# ---------- Training loop with Procrustes + InfoNCE ----------\n",
        "def training(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
        "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07,\n",
        "             queue_size=4098):\n",
        "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # --- Optional: Procrustes initialization ---\n",
        "    if use_procrustes_init:\n",
        "        print(\"Computing Procrustes initialization...\")\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
        "\n",
        "    # --- Training ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
        "            loss += 1 - F.cosine_similarity(\n",
        "                F.normalize(pred, dim=-1),\n",
        "                F.normalize(y_batch, dim=-1)\n",
        "            ).mean()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
        "              # put them into the queue\n",
        "              criterion._enqueue(keys)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                pred = model(X_batch)\n",
        "                loss = criterion(pred, y_batch)\n",
        "                loss += 1 - F.cosine_similarity(\n",
        "                    F.normalize(pred, dim=-1),\n",
        "                    F.normalize(y_batch, dim=-1)\n",
        "                ).mean()\n",
        "                loss += 0.1 * F.mse_loss(pred, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
        "                criterion._enqueue(keys)\n",
        "\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        # --- Save best model ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e41a65",
      "metadata": {
        "id": "c4e41a65"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1a3e15",
      "metadata": {
        "id": "fe1a3e15",
        "outputId": "b3f6bc75-3ba6-4bb9-90de-e5f35d736ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000,)\n",
            "Train data: 125000 captions, 125000 images\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([112500, 1536]), torch.Size([112500, 1024]), 512, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 4. Data Augmentation\n",
        "# 5. Zero Shot Stitching\n",
        "# 6. Triplet Loss / Improve InfoNCE Loss / bidirectional / SimCLR / MoCo\n",
        "# 7. Autoencoder\n",
        "# Configuration\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512\n",
        "LR = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load data\n",
        "train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
        "#train_data = load_data('processed_augmented_data.npz')\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4bed17",
      "metadata": {
        "id": "2e4bed17"
      },
      "source": [
        "### Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45828c5",
      "metadata": {
        "id": "f45828c5",
        "outputId": "dabf8562-27c0-4a49-c994-bcfc1eedb78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46d6b64",
      "metadata": {
        "id": "b46d6b64"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE):\n",
        "\n",
        "    # --- Common hyperparameters ---\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    #lr = trial.suggest_loguniform(\"lr\", 5e-4, 1e-2)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
        "\n",
        "    # --- New hyperparameters ---\n",
        "    temperature = trial.suggest_float(\"temperature\", 0.01, 0.2)\n",
        "    queue_size = trial.suggest_categorical(\"queue_size\", [2048, 4098, 8196])\n",
        "    #w_infonce = trial.suggest_float(\"w_infonce\", 0.6, 0.8)\n",
        "    #w_cos = trial.suggest_float(\"w_cos\", 0.4, 1.0)\n",
        "    #w_mse = trial.suggest_float(\"w_mse\", 1.0 - w_cos, 1.0)\n",
        "    procrustes_subset = 10000\n",
        "\n",
        "    # --- Architecture-specific hyperparameters ---\n",
        "    if arch in [\"MLP\", \"ResidualMLP\"]:\n",
        "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [1024, 2048, 4096])\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
        "        if arch == \"MLP\":\n",
        "            model = LatentSpaceTranslator(\n",
        "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers, dropout=dropout\n",
        "            ).to(device)\n",
        "        else:\n",
        "            model = ResidualMLPTranslator(\n",
        "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers, dropout=dropout\n",
        "            ).to(device)\n",
        "    elif arch == \"Transformer\":\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
        "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8, 12])\n",
        "        dim_feedforward = trial.suggest_categorical(\"dim_feedforward\", [1024, 2048, 4096])\n",
        "        model = TransformerTranslator(\n",
        "            text_dim=1024, img_dim=1536,\n",
        "            n_heads=n_heads, n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "    # --- Apply Procrustes initialization ---\n",
        "    if procrustes_subset > 0:\n",
        "        # Get subset from train_loader\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
        "    # --- Training loop (short run) ---\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    for epoch in range(5):  # short training\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "\n",
        "            # Weighted combination of losses\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
        "            loss += 1 - F.cosine_similarity(\n",
        "                F.normalize(pred, dim=-1),\n",
        "                F.normalize(y_batch, dim=-1)\n",
        "            ).mean()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
        "              # put them into the queue\n",
        "              criterion._enqueue(keys)\n",
        "\n",
        "    # --- Evaluate on validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            # Use combined loss for evaluation\n",
        "            loss =  criterion(pred, y_batch)\n",
        "            loss += 0.1 * F.mse_loss(pred, y_batch)\n",
        "            loss += 1 - F.cosine_similarity(\n",
        "                F.normalize(pred, dim=-1),\n",
        "                F.normalize(y_batch, dim=-1)\n",
        "            ).mean()\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            keys = F.normalize(y_batch, dim=1).detach()\n",
        "            criterion._enqueue(keys)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def run_optuna_extended(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=20):\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
        "                   n_trials=n_trials)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"Val loss: {trial.value}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8d430c",
      "metadata": {
        "id": "dc8d430c",
        "outputId": "335cd6de-fc40-4a3a-93b3-bf2079614b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:52:22,050] A new study created in memory with name: no-name-08bd2d31-237f-48ab-9364-370672aaa92f\n",
            "/tmp/ipython-input-4063544464.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:52:55,531] Trial 0 finished with value: 7.416419296264649 and parameters: {'dropout': 0.49742800954972666, 'weight_decay': 7.362804329068901e-05, 'temperature': 0.12011311049891497, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 2}. Best is trial 0 with value: 7.416419296264649.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:53:58,284] Trial 1 finished with value: 8.070483894348145 and parameters: {'dropout': 0.4457451145579906, 'weight_decay': 9.98629580313025e-05, 'temperature': 0.12247135628417309, 'queue_size': 8196, 'hidden_dim': 4096, 'num_layers': 4}. Best is trial 0 with value: 7.416419296264649.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:54:41,522] Trial 2 finished with value: 7.832575187683106 and parameters: {'dropout': 0.43633819649567396, 'weight_decay': 0.00017283443986310703, 'temperature': 0.06532048777796587, 'queue_size': 8196, 'hidden_dim': 1024, 'num_layers': 6}. Best is trial 0 with value: 7.416419296264649.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:55:24,656] Trial 3 finished with value: 8.097517204284667 and parameters: {'dropout': 0.23948322889101303, 'weight_decay': 6.1016513016738906e-05, 'temperature': 0.18259822020723201, 'queue_size': 4098, 'hidden_dim': 1024, 'num_layers': 6}. Best is trial 0 with value: 7.416419296264649.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:56:54,419] Trial 4 finished with value: 6.630535087585449 and parameters: {'dropout': 0.33752287263810776, 'weight_decay': 7.580403534293815e-05, 'temperature': 0.1233813430882792, 'queue_size': 2048, 'hidden_dim': 4096, 'num_layers': 6}. Best is trial 4 with value: 6.630535087585449.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:57:36,026] Trial 5 finished with value: 7.159534511566162 and parameters: {'dropout': 0.31396002655223965, 'weight_decay': 1.2806989471404e-05, 'temperature': 0.16953877508938006, 'queue_size': 2048, 'hidden_dim': 2048, 'num_layers': 4}. Best is trial 4 with value: 6.630535087585449.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:58:52,499] Trial 6 finished with value: 5.311068687438965 and parameters: {'dropout': 0.2432517166236121, 'weight_decay': 0.0009789028553740663, 'temperature': 0.03251864033104476, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 6 with value: 5.311068687438965.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 14:59:35,025] Trial 7 finished with value: 7.671458511352539 and parameters: {'dropout': 0.43715785342701075, 'weight_decay': 4.644806715835174e-05, 'temperature': 0.13451867452206884, 'queue_size': 4098, 'hidden_dim': 2048, 'num_layers': 4}. Best is trial 6 with value: 5.311068687438965.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-12 15:00:03,725] Trial 8 finished with value: 7.245289916992188 and parameters: {'dropout': 0.25049756146382074, 'weight_decay': 2.9150702437505698e-05, 'temperature': 0.16110979290539296, 'queue_size': 2048, 'hidden_dim': 1024, 'num_layers': 2}. Best is trial 6 with value: 5.311068687438965.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:00:39,698] Trial 9 finished with value: 7.070106239318847 and parameters: {'dropout': 0.33223475305502115, 'weight_decay': 0.00026378285850688017, 'temperature': 0.15305827019631665, 'queue_size': 2048, 'hidden_dim': 2048, 'num_layers': 3}. Best is trial 6 with value: 5.311068687438965.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:01:56,464] Trial 10 finished with value: 5.250538902282715 and parameters: {'dropout': 0.1253286954011852, 'weight_decay': 0.0008734229618288476, 'temperature': 0.032894900219331995, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 10 with value: 5.250538902282715.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:03:12,939] Trial 11 finished with value: 4.617178039550781 and parameters: {'dropout': 0.10275304393218701, 'weight_decay': 0.0008585524239400026, 'temperature': 0.01734322931419159, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 11 with value: 4.617178039550781.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:04:29,736] Trial 12 finished with value: 4.504851589202881 and parameters: {'dropout': 0.11977980314311326, 'weight_decay': 0.0009158729366245157, 'temperature': 0.0130772111143477, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:05:46,416] Trial 13 finished with value: 6.44474042892456 and parameters: {'dropout': 0.10239877988229126, 'weight_decay': 0.00043574335755792564, 'temperature': 0.07304286581347741, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:07:03,206] Trial 14 finished with value: 4.544445514678955 and parameters: {'dropout': 0.15688464500944224, 'weight_decay': 0.0004669591807778276, 'temperature': 0.011522813334605626, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:08:05,813] Trial 15 finished with value: 6.496157341003418 and parameters: {'dropout': 0.17952664362318446, 'weight_decay': 0.0004094312384318047, 'temperature': 0.07290547254930618, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 4}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:08:55,427] Trial 16 finished with value: 5.30661470413208 and parameters: {'dropout': 0.16850797721728147, 'weight_decay': 0.00042851347174174256, 'temperature': 0.013778981454863587, 'queue_size': 8196, 'hidden_dim': 4096, 'num_layers': 3}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:10:11,775] Trial 17 finished with value: 5.898998985290527 and parameters: {'dropout': 0.18061606396503355, 'weight_decay': 0.00022887377182079975, 'temperature': 0.050473550011568336, 'queue_size': 4098, 'hidden_dim': 4096, 'num_layers': 5}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:10:44,262] Trial 18 finished with value: 7.328660087585449 and parameters: {'dropout': 0.14633639022426423, 'weight_decay': 0.0005490886370031927, 'temperature': 0.09394935107328228, 'queue_size': 4098, 'hidden_dim': 1024, 'num_layers': 3}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-12 15:11:39,497] Trial 19 finished with value: 6.750846462249756 and parameters: {'dropout': 0.21359375316860474, 'weight_decay': 0.0001308505451126721, 'temperature': 0.043102430996840045, 'queue_size': 8196, 'hidden_dim': 2048, 'num_layers': 6}. Best is trial 12 with value: 4.504851589202881.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "Val loss: 4.504851589202881\n",
            "Best hyperparameters:\n",
            "  dropout: 0.11977980314311326\n",
            "  weight_decay: 0.0009158729366245157\n",
            "  temperature: 0.0130772111143477\n",
            "  queue_size: 4098\n",
            "  hidden_dim: 4096\n",
            "  num_layers: 5\n"
          ]
        }
      ],
      "source": [
        "archs = ['MLP', 'ResidualMLP', 'Transformer']\n",
        "choosen_arch = archs[1]\n",
        "best_params = run_optuna_extended(\n",
        "    arch = choosen_arch,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    device=DEVICE,\n",
        "    MODEL_PATH_BASE=\"models/translator_optuna\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e38bcc",
      "metadata": {
        "id": "98e38bcc"
      },
      "source": [
        "### Training and Submission File Creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if choosen_arch == 'Transformer':\n",
        "    model = TransformerTranslator(\n",
        "        text_dim=1024,\n",
        "        img_dim=1536,\n",
        "        n_heads = best_params['n_heads'],\n",
        "        n_layers=best_params['n_layers'],\n",
        "        dim_feedforward=best_params['dim_feedforward'],\n",
        "        dropout=best_params['dropout']\n",
        "    ).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data//models/transformer.pth\"\n",
        "\n",
        "elif choosen_arch == 'MLP':\n",
        "    model = LatentSpaceTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/latent_space.pth\"\n",
        "\n",
        "else:\n",
        "    model = ResidualMLPTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/residual.pth\""
      ],
      "metadata": {
        "id": "p0_hDfyGIngh"
      },
      "id": "p0_hDfyGIngh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204d7a49",
      "metadata": {
        "id": "204d7a49",
        "outputId": "4ff8cdf8-19b1-498d-abf1-0029ec0a2332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Parameters: 62,447,616\n",
            "\n",
            "3. Training...\n",
            "Computing Procrustes initialization...\n",
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 220/220 [00:14<00:00, 14.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 4.526064, Val Loss = 4.254377\n",
            "  ✓ Saved best model (val_loss=4.254377)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 220/220 [00:14<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 3.815003, Val Loss = 3.991249\n",
            "  ✓ Saved best model (val_loss=3.991249)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 220/220 [00:14<00:00, 14.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 3.476233, Val Loss = 3.882286\n",
            "  ✓ Saved best model (val_loss=3.882286)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 220/220 [00:14<00:00, 14.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 3.233820, Val Loss = 3.812615\n",
            "  ✓ Saved best model (val_loss=3.812615)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 220/220 [00:14<00:00, 14.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 3.036878, Val Loss = 3.766588\n",
            "  ✓ Saved best model (val_loss=3.766588)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 220/220 [00:14<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss = 2.873553, Val Loss = 3.735478\n",
            "  ✓ Saved best model (val_loss=3.735478)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 220/220 [00:14<00:00, 14.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss = 2.723539, Val Loss = 3.705102\n",
            "  ✓ Saved best model (val_loss=3.705102)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 220/220 [00:14<00:00, 14.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss = 2.593742, Val Loss = 3.693465\n",
            "  ✓ Saved best model (val_loss=3.693465)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 220/220 [00:14<00:00, 14.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss = 2.468695, Val Loss = 3.685102\n",
            "  ✓ Saved best model (val_loss=3.685102)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 220/220 [00:14<00:00, 15.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss = 2.362414, Val Loss = 3.687659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 220/220 [00:14<00:00, 14.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss = 2.257286, Val Loss = 3.679179\n",
            "  ✓ Saved best model (val_loss=3.679179)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 220/220 [00:14<00:00, 14.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Loss = 2.157557, Val Loss = 3.689908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 220/220 [00:14<00:00, 14.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Loss = 2.062617, Val Loss = 3.694384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 220/220 [00:14<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss = 1.977105, Val Loss = 3.700991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 220/220 [00:14<00:00, 14.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Loss = 1.895088, Val Loss = 3.708466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 220/220 [00:14<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss = 1.812696, Val Loss = 3.725518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100:  25%|██▌       | 55/220 [00:03<00:10, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-157252546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n3. Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = training(model,\n\u001b[0m\u001b[1;32m      6\u001b[0m                  \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                  \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1692327605.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH, use_procrustes_init, procrustes_subset, temperature, queue_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             ).mean()\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train\n",
        "print(\"\\n3. Training...\")\n",
        "model = training(model,\n",
        "                 train_loader,\n",
        "                 val_loader,\n",
        "                 DEVICE,\n",
        "                 EPOCHS,\n",
        "                 1e-5,\n",
        "                 MODEL_PATH,\n",
        "                 True,\n",
        "                 10000,\n",
        "                 best_params[\"temperature\"],\n",
        "                 best_params[\"queue_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff97f7b",
      "metadata": {
        "id": "cff97f7b",
        "outputId": "3e615aa8-1d46-4bdc-fbac-6a59e67a910d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission file...\n",
            "✓ Saved submission to ResidualMLP_submission.csv\n",
            "Model saved to: drive/MyDrive/data/models/residual.pth\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], pred_embds, f'{choosen_arch}_submission.csv')\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KKSa0Jl55hd"
      },
      "id": "_KKSa0Jl55hd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}