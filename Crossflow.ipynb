{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fbb1136c",
      "metadata": {
        "id": "fbb1136c"
      },
      "source": [
        "## Notebook for the Crossflow Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab770d75",
      "metadata": {
        "id": "ab770d75"
      },
      "source": [
        "### Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "17bb3be3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bb3be3",
        "outputId": "b8cf5160-b7f5-4c06-9619-001601e17382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'challenge' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#!mkdir data\n",
        "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "#!unzip -o test.zip -d data\n",
        "#!unzip -o train.zip -d data\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "!git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "80e4232b",
      "metadata": {
        "id": "80e4232b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from challenge.src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df810e23",
      "metadata": {
        "id": "df810e23"
      },
      "source": [
        "### Create Neural Network Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370a7603",
      "metadata": {
        "id": "370a7603"
      },
      "source": [
        "- VAE-ENCODER (1024) -> LATENT SPACE (1536) -> VAE-DECODER (1024) train a VAE in parallel with the crossflow network\n",
        "- CROSSFLOW GETS THE LATENT SPACE FROM VAE AS INPUT\n",
        "-> INPUT (1024) -> VAE-ENCODER (1536) -> INPUT FOR CROSSFLOW -> CROSSFLOW TRANSFORMER -> OUTPUT FOR CROSSFLOW (1536)\n",
        "- Use Text Embeddings as input for vae and image embeddings for crossflow\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "413e8c03",
      "metadata": {
        "id": "413e8c03"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Simple MLP block with LayerNorm, activation, and dropout\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, dropout=0.0, activation=nn.GELU):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.LayerNorm(in_dim),\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            activation(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ContextMapperVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Configurable VAE:\n",
        "    - input_dim: dimension of the context embeddings\n",
        "    - latent_dim: dimension of the target/image latent\n",
        "    - num_layers: number of hidden layers for encoder/decoder\n",
        "    - hidden_dim: width of hidden layers\n",
        "    - dropout: dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, latent_dim, num_layers=6, hidden_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # ---------------- Encoder ----------------\n",
        "        enc_layers = []\n",
        "        dim_in = input_dim\n",
        "        for _ in range(num_layers):\n",
        "            enc_layers.append(MLPBlock(dim_in, hidden_dim, dropout))\n",
        "            dim_in = hidden_dim\n",
        "        self.encoder_backbone = nn.Sequential(*enc_layers)\n",
        "        self.encoder_head = nn.Linear(hidden_dim, latent_dim * 2)  # μ and logσ\n",
        "\n",
        "        # ---------------- Decoder ----------------\n",
        "        dec_layers = []\n",
        "        dim_in = latent_dim\n",
        "        for _ in range(num_layers):\n",
        "            dec_layers.append(MLPBlock(dim_in, hidden_dim, dropout))\n",
        "            dim_in = hidden_dim\n",
        "        self.decoder_backbone = nn.Sequential(*dec_layers)\n",
        "        self.decoder_head = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- Encoder path ---\n",
        "        h = self.encoder_backbone(x)\n",
        "        stats = self.encoder_head(h)\n",
        "        mu, log_sigma = stats.chunk(2, dim=-1)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        eps = torch.randn_like(mu)\n",
        "        z0 = mu + sigma * eps\n",
        "\n",
        "        # --- Decoder path ---\n",
        "        h_dec = self.decoder_backbone(z0)\n",
        "        x_recon = self.decoder_head(h_dec)\n",
        "        return z0, mu, log_sigma, x_recon\n",
        "\n",
        "    def kl_loss(self, mu, log_sigma):\n",
        "        # KL(q(z|x) || N(0,1))\n",
        "        return -0.5 * torch.sum(1 + 2 * log_sigma - mu.pow(2) - torch.exp(2 * log_sigma), dim=-1).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce2476cb",
      "metadata": {
        "id": "ce2476cb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# --- Classi di Supporto per il U-ViT ---\n",
        "\n",
        "class TimestepEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Crea embedding sinusoidali per il tempo e li proietta con un MLP.\n",
        "    Questo è un modo standard e potente per condizionare sul tempo.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "        )\n",
        "        self.freq_embed = nn.Parameter(torch.randn(1, embed_dim // 2), requires_grad=False)\n",
        "\n",
        "    def forward(self, t):\n",
        "        # Crea l'embedding sinusoidale\n",
        "        t_freq = t.unsqueeze(-1) * self.freq_embed.to(t.device) * 2 * math.pi\n",
        "        emb = torch.cat([torch.sin(t_freq), torch.cos(t_freq)], dim=-1)\n",
        "        \n",
        "        # Proietta attraverso l'MLP\n",
        "        return self.mlp(emb)\n",
        "\n",
        "class AdaLNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Layer Normalization.\n",
        "    Usa l'embedding temporale per calcolare scale e shift.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, time_embed_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
        "        self.proj = nn.Linear(time_embed_dim, 2 * embed_dim)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Proietta l'embedding temporale per ottenere scale e shift\n",
        "        scale, shift = self.proj(time_emb).chunk(2, dim=-1)\n",
        "        # Applica la modulazione: y = scale * norm(x) + shift\n",
        "        # Aggiungiamo le dimensioni necessarie per il broadcasting\n",
        "        return self.norm(x) * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "class UViTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Un singolo blocco del nostro U-ViT.\n",
        "    Contiene Self-Attention e un Feed-Forward, entrambi modulati dal tempo.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, time_embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = AdaLNorm(embed_dim, time_embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        \n",
        "        self.norm2 = AdaLNorm(embed_dim, time_embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Blocco di attenzione con connessione residua\n",
        "        x = x + self.attn(self.norm1(x, time_emb), self.norm1(x, time_emb), self.norm1(x, time_emb))[0]\n",
        "        # Blocco feed-forward con connessione residua\n",
        "        x = x + self.ffn(self.norm2(x, time_emb))\n",
        "        return x\n",
        "\n",
        "# --- La Nuova Architettura: UViTFlow ---\n",
        "\n",
        "class UViTFlow(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Flow basato su architettura U-ViT.\n",
        "    Prende un vettore latente, lo rimodella in un formato spaziale,\n",
        "    lo processa con un'architettura U-Net di blocchi Transformer,\n",
        "    e lo riappiattisce in un vettore.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=1536, channels=24, size=8, patch_size=2,\n",
        "                 embed_dim=768, depth=6, num_heads=8, ff_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert latent_dim == channels * size * size, \"Dimensioni latenti non compatibili con la forma spaziale\"\n",
        "        assert size % patch_size == 0, \"La dimensione deve essere divisibile per la patch_size\"\n",
        "        \n",
        "        self.latent_dim = latent_dim\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (size // patch_size) ** 2\n",
        "\n",
        "        # 1. Embedding per il tempo\n",
        "        self.time_embed = TimestepEmbedding(256, 1024, embed_dim)\n",
        "\n",
        "        # 2. Patch Embedding\n",
        "        self.patch_embed = nn.Conv2d(channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
        "\n",
        "        # 3. Blocchi U-ViT (Encoder + Decoder con skip connections)\n",
        "        self.depth = depth\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            UViTBlock(embed_dim, num_heads, embed_dim, ff_dim, dropout) for _ in range(depth)\n",
        "        ])\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            UViTBlock(embed_dim, num_heads, embed_dim, ff_dim, dropout) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # 4. Layer finale\n",
        "        self.final_norm = nn.LayerNorm(embed_dim)\n",
        "        self.final_proj = nn.Linear(embed_dim, channels * patch_size * patch_size)\n",
        "\n",
        "    def forward(self, z_t, t):\n",
        "        # --- Preparazione Input ---\n",
        "        # 0. Rimodella il vettore in un formato spaziale\n",
        "        # Input z_t: [B, 1536] -> [B, 24, 8, 8]\n",
        "        x = z_t.view(-1, self.channels, self.size, self.size)\n",
        "\n",
        "        # 1. Calcola l'embedding temporale\n",
        "        # Input t: [B] -> time_emb: [B, embed_dim]\n",
        "        time_emb = self.time_embed(t)\n",
        "\n",
        "        # --- Encoder U-ViT ---\n",
        "        # 2. Patch & Position Embedding\n",
        "        # x: [B, 24, 8, 8] -> [B, 768, 4, 4]\n",
        "        x = self.patch_embed(x)\n",
        "        # x: [B, 768, 4, 4] -> [B, 16, 768] (flatten spaziale)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # 3. Applica i blocchi dell'encoder, salvando gli output per le skip connections\n",
        "        skip_connections = []\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x, time_emb)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # --- Decoder U-ViT ---\n",
        "        # 4. Applica i blocchi del decoder, aggiungendo le skip connections\n",
        "        for block in self.decoder_blocks:\n",
        "            x = x + skip_connections.pop() # Aggiunge la skip connection\n",
        "            x = block(x, time_emb)\n",
        "\n",
        "        # --- Output ---\n",
        "        # 5. Proiezione finale per tornare allo spazio originale\n",
        "        x = self.final_norm(x)\n",
        "        x = self.final_proj(x)\n",
        "        \n",
        "        # 6. \"Un-patch\" e appiattimento\n",
        "        # x: [B, 16, 24*2*2] -> [B, 16, 96]\n",
        "        # Ricostruisci la forma spaziale [B, C, H, W]\n",
        "        # Questo è l'inverso del patch embedding\n",
        "        num_patches_h = self.size // self.patch_size\n",
        "        x = x.transpose(1, 2).view(-1, self.channels, self.patch_size, self.patch_size, num_patches_h, num_patches_h)\n",
        "        x = x.permute(0, 1, 4, 2, 5, 3).reshape(-1, self.channels, self.size, self.size)\n",
        "        \n",
        "        # 7. Appiattisci l'output per tornare a un vettore\n",
        "        # x: [B, 24, 8, 8] -> [B, 1536]\n",
        "        v_pred = x.view(-1, self.latent_dim)\n",
        "        \n",
        "        return v_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dcb54213",
      "metadata": {
        "id": "dcb54213"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def integrate_flow(flow, z0, n_steps=20):\n",
        "    z = z0.clone()\n",
        "    t_values = torch.linspace(0, 1, n_steps, device=z0.device)\n",
        "    dt = 1.0 / n_steps\n",
        "    for t in t_values:\n",
        "        v = flow(z, t.repeat(z.size(0)))\n",
        "        z = z + dt * v\n",
        "    return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c02bece2",
      "metadata": {
        "id": "c02bece2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def train_epoch(train_loader, vae, flow, optimizer,\n",
        "                lambda_kl=1e-4,\n",
        "                temperature=0.07, queue_size=4098, device=\"cuda\", epoch = 0):\n",
        "    vae.train()\n",
        "    flow.train()\n",
        "    total_loss, total_fm, total_enc, total_kl = 0, 0, 0, 0\n",
        "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch: {epoch}\"):\n",
        "        context = X_batch.to(device)\n",
        "        image = y_batch.to(device)\n",
        "\n",
        "        z0, mu, log_sigma, recon = vae(context)\n",
        "        z1 = image\n",
        "\n",
        "        t = torch.rand(z0.size(0), 1, device=device)\n",
        "        z_t = (1 - t) * z0 + t * z1\n",
        "        v_hat = z1 - z0\n",
        "        v_pred = flow(z_t, t.squeeze())\n",
        "\n",
        "        L_Enc = criterion(z0, z1)\n",
        "        L_FM = F.mse_loss(v_pred, v_hat)\n",
        "        L_KL = vae.kl_loss(mu, log_sigma)\n",
        "\n",
        "        loss = L_FM + L_Enc + lambda_kl * L_KL\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_fm += L_FM.item()\n",
        "        total_enc += L_Enc.item()\n",
        "        total_kl += L_KL.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          img_keys = F.normalize(image, dim=1).detach()\n",
        "          # put them into the queue\n",
        "          criterion._enqueue(keys=img_keys)\n",
        "\n",
        "    n = len(train_loader)\n",
        "    return {\n",
        "        \"loss\": total_loss / n,\n",
        "        \"L_FM\": total_fm / n,\n",
        "        \"L_Enc\": total_enc / n,\n",
        "        \"L_KL\": total_kl / n\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(val_loader, vae, flow, device=\"cuda\", n_steps=20):\n",
        "    vae.eval()\n",
        "    flow.eval()\n",
        "    cos_sims = []\n",
        "\n",
        "    for X_batch, y_batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "        context = X_batch.to(device)\n",
        "        image = y_batch.to(device)\n",
        "\n",
        "        # Encode to z0\n",
        "        z0, _, _, _ = vae(context)\n",
        "\n",
        "        # Integrate flow to predict target embedding\n",
        "        z1_pred = integrate_flow(flow, z0, n_steps=n_steps)\n",
        "        z1_true = image\n",
        "\n",
        "        # Compute cosine similarity between predicted and true image embeddings\n",
        "        cos_sim = F.cosine_similarity(z1_pred, z1_true, dim=-1)\n",
        "        cos_sims.append(cos_sim.cpu().numpy())\n",
        "\n",
        "    cos_sims = np.concatenate(cos_sims)\n",
        "    mean_cosine = np.mean(cos_sims)\n",
        "    acc_80 = np.mean(cos_sims > 0.8)  # how often similarity > 0.8\n",
        "    acc_90 = np.mean(cos_sims > 0.9)\n",
        "\n",
        "    return {\n",
        "        \"mean_cosine\": mean_cosine,\n",
        "        \"acc@0.8\": acc_80,\n",
        "        \"acc@0.9\": acc_90\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "uD7jgHRIxASO",
      "metadata": {
        "id": "uD7jgHRIxASO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class QueueInfoNCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    One-directional (text → image) InfoNCE with optional sinusoidal time embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, temperature=0.07, queue_size=4096, use_time_embedding=True):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.queue_size = queue_size\n",
        "\n",
        "        # Single queue for image embeddings\n",
        "        self.register_buffer(\"queue\", F.normalize(torch.randn(queue_size, dim), dim=1))\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------\n",
        "    # Queue Management\n",
        "    # ------------------------------\n",
        "    @torch.no_grad()\n",
        "    def _enqueue(self, keys):\n",
        "        \"\"\"Add image keys (B, dim) to the circular queue after backward().\"\"\"\n",
        "        bsz = keys.shape[0]\n",
        "        keys = F.normalize(keys, dim=1)\n",
        "\n",
        "        ptr = int(self.queue_ptr.item())\n",
        "        end_ptr = (ptr + bsz) % self.queue_size\n",
        "\n",
        "        if end_ptr > ptr:\n",
        "            self.queue[ptr:end_ptr] = keys\n",
        "        else:\n",
        "            first_len = self.queue_size - ptr\n",
        "            self.queue[ptr:] = keys[:first_len]\n",
        "            self.queue[:end_ptr] = keys[first_len:]\n",
        "\n",
        "        self.queue_ptr[0] = end_ptr\n",
        "\n",
        "    # ------------------------------\n",
        "    # Forward Pass\n",
        "    # ------------------------------\n",
        "    def forward(self, z_text, z_img):\n",
        "        \"\"\"\n",
        "        z_text: (B, dim) predicted text→image latent\n",
        "        z_img:  (B, dim) target image latent\n",
        "        \"\"\"\n",
        "\n",
        "        # Normalize embeddings\n",
        "        z_text = F.normalize(z_text, dim=1)\n",
        "        z_img = F.normalize(z_img, dim=1)\n",
        "\n",
        "        # Positive logits: (B, 1)\n",
        "        l_pos = torch.sum(z_text * z_img, dim=-1, keepdim=True)\n",
        "\n",
        "        # Negatives: (B, queue_size)\n",
        "        l_neg = torch.matmul(z_text, self.queue.T)\n",
        "\n",
        "        # Combine and scale by temperature\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / self.temperature\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=z_text.device)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4A2RR3C540kv",
      "metadata": {
        "id": "4A2RR3C540kv"
      },
      "outputs": [],
      "source": [
        "# ====== Procrustes initialization ======\n",
        "def procrustes_init(text_embs, img_embs):\n",
        "    \"\"\"\n",
        "    text_embs: (N, d_text)\n",
        "    img_embs:  (N, d_img)\n",
        "    returns: weight matrix (d_img, d_text)\n",
        "    \"\"\"\n",
        "    # Center both\n",
        "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
        "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
        "\n",
        "    # Compute SVD of cross-covariance\n",
        "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
        "    W = U @ Vt  # orthogonal map d_text→d_img\n",
        "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
        "\n",
        "\n",
        "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
        "    \"\"\"\n",
        "    Apply Procrustes initialization to a model\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Compute Procrustes matrix\n",
        "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
        "\n",
        "        # Apply to the appropriate layer\n",
        "        applied = False\n",
        "        for name, m in model.named_modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Transformer: apply to first projection (proj_in)\n",
        "                if isinstance(model, TransformerFlow) and name.endswith(\"output\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "        if not applied:\n",
        "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a1104d",
      "metadata": {
        "id": "72a1104d"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ac1facf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac1facf2",
        "outputId": "80ff53e7-4858-4c7e-dc76-0c0aed56fe87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(125000,)\n",
            "Train data: 125000 captions, 125000 images\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([112500, 1536]), torch.Size([112500, 1024]), 256, 256)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Crossflow\n",
        "# 4. Data Augmentation\n",
        "# 5. Zero Shot Stitching\n",
        "# 6. Diffusion Priors\n",
        "# Configuration\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.0001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load data\n",
        "train_data = load_data(\"./data/train/train.npz\")\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0da21fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c0da21fd",
        "outputId": "51309fdc-756d-4717-821e-a85e5206edab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 0: 100%|██████████| 440/440 [02:32<00:00,  2.88it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/60\n",
            "Train: {'loss': 6.5542689182541585, 'L_FM': 0.30120736418122596, 'L_Enc': 6.02543553980914, 'L_KL': 2276.2602645007046}\n",
            "Val: {'mean_cosine': np.float32(0.7561125), 'acc@0.8': np.float64(0.21416), 'acc@0.9': np.float64(8e-05)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 1: 100%|██████████| 440/440 [02:27<00:00,  2.99it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/60\n",
            "Train: {'loss': 5.676858457652005, 'L_FM': 0.19580019099468535, 'L_Enc': 5.220968061143702, 'L_KL': 2600.902276056463}\n",
            "Val: {'mean_cosine': np.float32(0.7567622), 'acc@0.8': np.float64(0.22808), 'acc@0.9': np.float64(0.00096)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 2: 100%|██████████| 440/440 [02:31<00:00,  2.90it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:44<00:00,  2.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/60\n",
            "Train: {'loss': 5.422777050191706, 'L_FM': 0.17212211608209393, 'L_Enc': 4.983886927366257, 'L_KL': 2667.680331143466}\n",
            "Val: {'mean_cosine': np.float32(0.76182187), 'acc@0.8': np.float64(0.25984), 'acc@0.9': np.float64(0.0016)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 3: 100%|██████████| 440/440 [02:33<00:00,  2.86it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:46<00:00,  2.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/60\n",
            "Train: {'loss': 5.2583437702872535, 'L_FM': 0.15989220941608603, 'L_Enc': 4.826927531849254, 'L_KL': 2715.2403875177556}\n",
            "Val: {'mean_cosine': np.float32(0.7652783), 'acc@0.8': np.float64(0.28624), 'acc@0.9': np.float64(0.0028)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 4: 100%|██████████| 440/440 [02:24<00:00,  3.05it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:38<00:00,  2.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/60\n",
            "Train: {'loss': 5.1312492094256665, 'L_FM': 0.15177169100127436, 'L_Enc': 4.704134620319714, 'L_KL': 2753.429152610085}\n",
            "Val: {'mean_cosine': np.float32(0.76829845), 'acc@0.8': np.float64(0.3124), 'acc@0.9': np.float64(0.00392)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 5: 100%|██████████| 440/440 [02:28<00:00,  2.96it/s]\n",
            "Validation: 100%|██████████| 49/49 [01:42<00:00,  2.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/60\n",
            "Train: {'loss': 5.024801966818896, 'L_FM': 0.14530806277285924, 'L_Enc': 4.600688010996039, 'L_KL': 2788.0589976917613}\n",
            "Val: {'mean_cosine': np.float32(0.7679533), 'acc@0.8': np.float64(0.31496), 'acc@0.9': np.float64(0.00536)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 6:  99%|█████████▉| 436/440 [02:25<00:01,  3.04it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "vae = ContextMapperVAE(\n",
        "    input_dim=1024, latent_dim=1536,\n",
        "    num_layers=3, hidden_dim=1024, dropout=0.1\n",
        ").to(DEVICE)\n",
        "\n",
        "# I parametri sono scelti per essere potenti ma gestibili.\n",
        "# Puoi fare tuning su embed_dim, depth, num_heads, etc.\n",
        "flow = UViTFlow(\n",
        "    latent_dim=1536,    # Dimensione totale del vettore\n",
        "    channels=24,        # Canali della rappresentazione spaziale (24*8*8=1536)\n",
        "    size=8,             # Altezza/Larghezza della rappresentazione spaziale\n",
        "    patch_size=2,       # Dimensione di ogni patch (8x8 -> 4x4 patches)\n",
        "    embed_dim=768,      # Dimensione interna del Transformer\n",
        "    depth=6,            # Numero di blocchi encoder/decoder\n",
        "    num_heads=12,       # Numero di teste di attenzione\n",
        "    ff_dim=3072,        # Dimensione del layer feed-forward (spesso 4*embed_dim)\n",
        "    dropout=0.1\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(vae.parameters()) + list(flow.parameters()),\n",
        "    lr=1e-4, weight_decay=0.03, betas=(0.9, 0.9)\n",
        ")\n",
        "\n",
        "procrustes_init = False\n",
        "if procrustes_init:\n",
        "  print(\"Computing Procrustes initialization...\")\n",
        "  text_list, img_list = [], []\n",
        "  for i, (X, y) in enumerate(train_loader):\n",
        "      text_list.append(X.cpu())\n",
        "      img_list.append(y.cpu())\n",
        "      if sum(t.shape[0] for t in text_list) >= 20000:\n",
        "            break\n",
        "  text_sample = torch.cat(text_list, dim=0)[:20000]\n",
        "  img_sample = torch.cat(img_list, dim=0)[:20000]\n",
        "  flow = apply_procrustes_init_to_final(flow, text_sample, img_sample)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_metrics = train_epoch(train_loader, vae, flow, optimizer, device=DEVICE, epoch=epoch)\n",
        "    val_metrics = validate_epoch(val_loader, vae, flow, device=DEVICE)\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"Train: {train_metrics}\")\n",
        "    print(f\"Val: {val_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2891bae2",
      "metadata": {
        "id": "2891bae2"
      },
      "source": [
        "### Training and Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1ed7c9",
      "metadata": {
        "id": "4b1ed7c9"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052ff0bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "052ff0bb",
        "outputId": "fa01dc80-b2de-4847-9290-d292f9086f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating submission file...\n",
            "✓ Saved submission to ./data/crossflow_submission.csv\n",
            "Model saved to: ./data/models/crossflow.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_data = load_data(\"./data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float().to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Encode to z0\n",
        "    z0, _, _, _ = vae(test_embds)\n",
        "    # Integrate flow to predict target embedding\n",
        "    z1_pred = integrate_flow(flow, z0, n_steps=50)\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], z1_pred, './data/crossflow_submission.csv')\n",
        "MODEL_PATH = \"./data/models/crossflow.pth\"\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BpGjy_8AsgGM",
      "metadata": {
        "id": "BpGjy_8AsgGM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca59fe33",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "MODEL_PATH = \"./data/models/crossflow.pth\"\n",
        "torch.save({\n",
        "    'vae_state_dict': vae.state_dict(),\n",
        "    'flow_state_dict': flow.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, MODEL_PATH)\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fds-challenge",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
