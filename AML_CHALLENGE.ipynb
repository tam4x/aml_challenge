{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryW8Z4mY8Vvo"
      },
      "source": [
        "# **AML Challenge: Model Stitching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQFGDwg7wLL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from PIL import Image\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7zDBTpl9fbd"
      },
      "source": [
        "## **Downloading the Text Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ3Fjmvr8e2c"
      },
      "outputs": [],
      "source": [
        "print(\"🫁 Downloading the model...\")\n",
        "text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBZ2YVHn-Cl8"
      },
      "source": [
        "### **Demo for the Text Encoder:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ9EcikI-Ghn"
      },
      "outputs": [],
      "source": [
        "text = \"A cat is hiding under the table\"\n",
        "\n",
        "# We obtain the \"embedding vector\" using the encode() function:\n",
        "emb = text_encoder.encode(text, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "# This is only for clarity:\n",
        "preview = emb.tolist()[:3] + [\"...\"] + emb.tolist()[-3:]\n",
        "print(f\"The embedding looks like this: {preview}\")\n",
        "\n",
        "# This is the shape of our embedding:\n",
        "print(f\"The shape of the embedding is: {emb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtIPGGaG9jl2"
      },
      "source": [
        "## **Downloading the VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJSK2xPX9bu1"
      },
      "outputs": [],
      "source": [
        "print(\"🫁 Downloading VAE from Hugging Face...\")\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe06iyg8-BXa"
      },
      "source": [
        "### **Demo for the VAE:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg060C8L_fVL"
      },
      "outputs": [],
      "source": [
        "IMG_URL = \"https://images.unsplash.com/photo-1574144611937-0df059b5ef3e?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=764\"\n",
        "img = Image.open(urlopen(IMG_URL)).convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), # --> keep this size fixed\n",
        "    # The VAE works also for 512x512, but it will require more compute\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "# Encode --> latent\n",
        "with torch.no_grad():\n",
        "    latents = vae.encode(img_tensor).latent_dist.sample() * 0.18215\n",
        "print(\"Latent shape:\", latents.shape)\n",
        "\n",
        "# Decode --> reconstruct\n",
        "with torch.no_grad():\n",
        "    recon = vae.decode(latents / 0.18215).sample\n",
        "\n",
        "recon = (recon.clamp(-1, 1) + 1) / 2\n",
        "recon_img = transforms.ToPILImage()(recon.squeeze().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6rxjK5zCDAr"
      },
      "outputs": [],
      "source": [
        "# Visualize input vs output:\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(img.resize((256, 256)))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(recon_img)\n",
        "axes[1].set_title(\"Reconstructed\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FudgLTdoGa5H"
      },
      "source": [
        "#**Frankenstein Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBkYpi5xGhoO"
      },
      "outputs": [],
      "source": [
        "class Translator(nn.Module):\n",
        "    \"\"\"\n",
        "    This will be the Translator Model you have to design for the challenge.\n",
        "    You have (almost) complete freedom on this. Your creativity will be rewarded.\n",
        "\n",
        "    Some ideas might be:\n",
        "    - Zero shot stitching (see https://arxiv.org/pdf/2209.15430)\n",
        "    - Linear, Affine, Orthognal solutions (see https://arxiv.org/pdf/2311.00664)\n",
        "    - Diffusion Priors (see https://arxiv.org/pdf/2204.06125)\n",
        "    - Flow Matching (see https://arxiv.org/pdf/2412.15213)\n",
        "    - CKA / Procrustes Analysis\n",
        "    - Adversarial Trainings\n",
        "    - AutoEncoding Solutions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Here is where *you* come into play:\n",
        "        self.fc = nn.Linear(384, 4 * 32 * 32)\n",
        "        # This is the most trivial thing you can do (spoiler: it doesn't work)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x.view(1, 4, 32, 32)\n",
        "\n",
        "translator = Translator().to(device)\n",
        "translator.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU_2vnkVGuNg"
      },
      "outputs": [],
      "source": [
        "# Part 1: encoding the text prompt\n",
        "text = \"Frankestein's Monster writing code on Google Colab\"\n",
        "print(f\"Prompt: {text}\")\n",
        "emb = text_encoder.encode(text, convert_to_tensor=True).to(device)\n",
        "print(\"Text embedding shape:\", emb.shape)\n",
        "\n",
        "# Part 2: translating the embedding\n",
        "with torch.no_grad():\n",
        "    latent = translator(emb)\n",
        "print(f\"Translated latent shape: {latent.shape}\\n\\n\")\n",
        "\n",
        "# Part 3: feed the VAE with the translation\n",
        "with torch.no_grad():\n",
        "    recon = vae.decode(latent / 0.18215).sample\n",
        "\n",
        "# Part 4: visualizing the output\n",
        "recon = (recon.clamp(-1, 1) + 1) / 2\n",
        "recon_img = transforms.ToPILImage()(recon.squeeze().cpu())\n",
        "\n",
        "plt.imshow(recon_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oi_AfL8tbcw"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XXzzq9AtaIl",
        "outputId": "afdb1301-b029-4372-cd42-964f7b8a2d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'challenge'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 98 (delta 39), reused 72 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 21.03 MiB | 17.81 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ],
      "source": [
        "#!mkdir data\n",
        "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
        "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
        "#!unzip -o test.zip -d data\n",
        "#!unzip -o train.zip -d data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/Mamiglia/challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kUZM092etlZw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from challenge.src.common import load_data, prepare_train_data, generate_submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-style translator from text embedding -> image embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, n_heads=8, n_layers=2, dim_feedforward=2048, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.proj_in = nn.Linear(text_dim, img_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=img_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True  # for (B, Seq, Dim)\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)  # (B, 1, text_dim)\n",
        "        x = self.input_ln(x)\n",
        "        x = self.proj_in(x)  # project to model dim\n",
        "        out = self.encoder(x)  # Transformer encoder\n",
        "        out = out.squeeze(1)   # remove sequence dim\n",
        "        return self.output_ln(out)"
      ],
      "metadata": {
        "id": "VtKo2gJdia3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualMLPTranslator(nn.Module):\n",
        "    def __init__(self, text_dim=1024, img_dim=1536, hidden_dim=2048, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # first layer: text_dim -> hidden_dim (no residual yet)\n",
        "        self.first_layer = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # hidden residual blocks (hidden_dim -> hidden_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            )\n",
        "            for _ in range(num_layers - 2)\n",
        "        ])\n",
        "\n",
        "        # final projection to image space\n",
        "        self.final_proj = nn.Linear(hidden_dim, img_dim)\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "        # input residual to output\n",
        "        if text_dim != img_dim:\n",
        "            self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "        else:\n",
        "            self.res_proj = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = self.input_ln(x)\n",
        "        out = self.first_layer(x_in)\n",
        "        for block in self.blocks:\n",
        "            out = out + block(out)  # residual only between same-dim layers\n",
        "        out = self.final_proj(out)\n",
        "        out = out + self.res_proj(x_in)\n",
        "        return self.output_ln(out)\n"
      ],
      "metadata": {
        "id": "UNbNXAz-7Rg3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentSpaceTranslator(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP translator from text embedding -> image embedding\n",
        "    Input: text_emb (batch, text_dim) or (batch, 1, text_dim)\n",
        "    Output: (batch, img_dim)\n",
        "    Regularization: dropout, LayerNorm, GELU, residual (optional projector)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 text_dim=1024,\n",
        "                 img_dim=1536,\n",
        "                 hidden_dim=2048,\n",
        "                 num_layers=3,\n",
        "                 dropout=0.2,\n",
        "                 use_residual=True):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2, \"num_layers should be >= 2 (including final proj)\"\n",
        "        self.use_residual = use_residual\n",
        "        self.input_ln = nn.LayerNorm(text_dim)\n",
        "        layers = []\n",
        "        in_dim = text_dim\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_dim = hidden_dim\n",
        "        # final projection to image space\n",
        "        layers.append(nn.Linear(in_dim, img_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # if using residual, project input to img_dim to add it at the end\n",
        "        if self.use_residual:\n",
        "            if text_dim != img_dim:\n",
        "                self.res_proj = nn.Linear(text_dim, img_dim)\n",
        "            else:\n",
        "                self.res_proj = nn.Identity()\n",
        "\n",
        "        # final layer norm in image space\n",
        "        self.output_ln = nn.LayerNorm(img_dim)\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        if text_emb.dim() == 3:\n",
        "            x = text_emb.squeeze(1)\n",
        "        else:\n",
        "            x = text_emb\n",
        "        x = self.input_ln(x)\n",
        "        out = self.net(x)  # (B, img_dim)\n",
        "        if self.use_residual:\n",
        "            res = self.res_proj(x)\n",
        "            out = out + res\n",
        "        return self.output_ln(out)\n"
      ],
      "metadata": {
        "id": "TSHxXkcQAKok"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training loop with Procrustes + InfoNCE ----------\n",
        "def training(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
        "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07,\n",
        "             w_nce = 0.2, w_mse = 0.2, w_cos = 0.2):\n",
        "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # --- Optional: Procrustes initialization ---\n",
        "    if use_procrustes_init:\n",
        "        print(\"Computing Procrustes initialization...\")\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    # --- Training ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(X_batch)\n",
        "            # Contrastive InfoNCE loss (can mix with cosine or MSE)\n",
        "            loss = w_nce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            # Optional mixed objective:\n",
        "            loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "            loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                pred = model(X_batch)\n",
        "                loss = w_nce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "                # Optional mixed objective:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        # --- Save best model ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3had6TwP7X5E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Procrustes initialization ======\n",
        "def procrustes_init(text_embs, img_embs):\n",
        "    \"\"\"\n",
        "    text_embs: (N, d_text)\n",
        "    img_embs:  (N, d_img)\n",
        "    returns: weight matrix (d_img, d_text)\n",
        "    \"\"\"\n",
        "    # Center both\n",
        "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
        "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
        "\n",
        "    # Compute SVD of cross-covariance\n",
        "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
        "    W = U @ Vt  # orthogonal map d_text→d_img\n",
        "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
        "\n",
        "# ====== InfoNCE (CLIP-style) loss ======\n",
        "def info_nce_loss(pred_img_emb, true_img_emb, temperature=0.07):\n",
        "    \"\"\"\n",
        "    pred_img_emb: (B, D)\n",
        "    true_img_emb: (B, D)\n",
        "    \"\"\"\n",
        "    zt = F.normalize(pred_img_emb, dim=1)\n",
        "    zi = F.normalize(true_img_emb, dim=1)\n",
        "    logits = zt @ zi.t() / temperature\n",
        "    labels = torch.arange(len(zt), device=zt.device)\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
        "    return 0.5 * (loss_i2t + loss_t2i)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
        "    \"\"\"\n",
        "    Apply Procrustes initialization to a model.\n",
        "    - For MLP / ResidualMLP: apply to final Linear layer (hidden -> img_dim)\n",
        "    - For TransformerTranslator: apply to first projection (text_dim -> img_dim)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Compute Procrustes matrix\n",
        "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
        "\n",
        "        # Apply to the appropriate layer\n",
        "        applied = False\n",
        "        for name, m in model.named_modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Transformer: apply to first projection (proj_in)\n",
        "                if isinstance(model, TransformerTranslator) and name.endswith(\"proj_in\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "                # MLP / ResidualMLP: apply to final_proj\n",
        "                elif isinstance(model, LatentSpaceTranslator) and name.endswith(\"res_proj\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "\n",
        "                elif isinstance(model, ResidualMLPTranslator) and name.endswith(\"res_proj\"):\n",
        "                    print(m.weight.shape, W.shape)\n",
        "                    if m.weight.shape == W.shape:\n",
        "                        m.weight.copy_(W)\n",
        "                        applied = True\n",
        "                        break\n",
        "\n",
        "        if not applied:\n",
        "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "gH43_GIXb7xg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHgZaxy_twIO",
        "outputId": "8e833ef6-ba1a-4f86-ed34-915ce772e88c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000,)\n",
            "Train data: 125000 captions, 125000 images\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([112500, 1536]), torch.Size([112500, 1024]), 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 1. Learning with a basic simple transformer\n",
        "# 2. Learning with a basic deep neural network\n",
        "# 3. TODO: Creat a hyperparameter optimization for the transformer aswell as\n",
        "# the lost function nce loss etc. and perform it on the first layer of the transformer\n",
        "# Configuration\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load data\n",
        "train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
        "X, y, label = prepare_train_data(train_data)\n",
        "DATASET_SIZE = len(X)\n",
        "# Split train/val\n",
        "# This is done only to measure generalization capabilities, you don't have to\n",
        "# use a validation set (though we encourage this)\n",
        "n_train = int(0.9 * len(X))\n",
        "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
        "TRAIN_SPLIT[:n_train] = 1\n",
        "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
        "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "hls7zZCTleL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "7s541MhBomzy",
        "outputId": "197b2f31-37d5-4753-805f-0a171a97721e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE):\n",
        "\n",
        "    # --- Common hyperparameters ---\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
        "\n",
        "    # --- New hyperparameters ---\n",
        "    temperature = trial.suggest_float(\"temperature\", 0.01, 0.2)\n",
        "    w_infonce = trial.suggest_float(\"w_infonce\", 0.6, 0.8)\n",
        "    w_cos = trial.suggest_float(\"w_cos\", 0.4, 1.0)\n",
        "    w_mse = trial.suggest_float(\"w_mse\", 1.0 - w_cos, 1.0)\n",
        "    procrustes_subset = 10000\n",
        "\n",
        "    # --- Architecture-specific hyperparameters ---\n",
        "    if arch in [\"MLP\", \"ResidualMLP\"]:\n",
        "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [1024, 2048, 4096])\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
        "        if arch == \"MLP\":\n",
        "            model = LatentSpaceTranslator(\n",
        "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers, dropout=dropout\n",
        "            ).to(device)\n",
        "        else:\n",
        "            model = ResidualMLPTranslator(\n",
        "                text_dim=1024, img_dim=1536, hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers, dropout=dropout\n",
        "            ).to(device)\n",
        "    elif arch == \"Transformer\":\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
        "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8, 12])\n",
        "        dim_feedforward = trial.suggest_categorical(\"dim_feedforward\", [1024, 2048, 4096])\n",
        "        model = TransformerTranslator(\n",
        "            text_dim=1024, img_dim=1536,\n",
        "            n_heads=n_heads, n_layers=n_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "    # --- Apply Procrustes initialization ---\n",
        "    if procrustes_subset > 0:\n",
        "        # Get subset from train_loader\n",
        "        text_list, img_list = [], []\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            text_list.append(X.cpu())\n",
        "            img_list.append(y.cpu())\n",
        "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
        "                break\n",
        "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
        "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
        "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
        "\n",
        "    # --- Training loop (short run) ---\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(5):  # short training\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "            pred = F.normalize(pred, dim=-1)\n",
        "            y_batch = F.normalize(y_batch, dim=-1)\n",
        "\n",
        "            # Weighted combination of losses\n",
        "            loss = 0\n",
        "            if w_infonce > 0:\n",
        "                loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            if w_mse > 0:\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            if w_cos > 0:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- Evaluate on validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            pred = F.normalize(pred, dim=-1)\n",
        "            y_batch = F.normalize(y_batch, dim=-1)\n",
        "            # Use combined loss for evaluation\n",
        "            loss = 0\n",
        "            if w_infonce > 0:\n",
        "                loss += w_infonce * info_nce_loss(pred, y_batch, temperature=temperature)\n",
        "            if w_mse > 0:\n",
        "                loss += w_mse * F.mse_loss(pred, y_batch)\n",
        "            if w_cos > 0:\n",
        "                loss += w_cos * (1 - F.cosine_similarity(pred, y_batch).mean())\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def run_optuna_extended(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=30):\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
        "                   n_trials=n_trials)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"Val loss: {trial.value}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return trial.params"
      ],
      "metadata": {
        "id": "QUlWlaSRldVd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archs = ['MLP', 'ResidualMLP', 'Transformer']\n",
        "choosen_arch = archs[2]\n",
        "best_params = run_optuna_extended(\n",
        "    arch = choosen_arch,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    device=DEVICE,\n",
        "    MODEL_PATH_BASE=\"models/translator_optuna\"\n",
        ")\n",
        "\n",
        "if choosen_arch == 'Transformer':\n",
        "    model = TransformerTranslator(\n",
        "        text_dim=1024,\n",
        "        img_dim=1536,\n",
        "        n_heads = best_params['n_heads'],\n",
        "        n_layers=best_params['n_layers'],\n",
        "        dim_feedforward=best_params['dim_feedforward'],\n",
        "        dropout=best_params['dropout']\n",
        "    ).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data//models/transformer.pth\"\n",
        "\n",
        "elif choosen_arch == 'MLP':\n",
        "    model = LatentSpaceTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/latent_space.pth\"\n",
        "\n",
        "else:\n",
        "    model = ResidualMLPTranslator(\n",
        "    text_dim=1024,\n",
        "    img_dim=1536,\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "    MODEL_PATH = \"drive/MyDrive/data/models/residual.pth\""
      ],
      "metadata": {
        "id": "ciC6HVDky2c1",
        "outputId": "7030c16f-2578-4e73-cab9-c535b8ecd3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-01 11:50:00,120] A new study created in memory with name: no-name-14f9d160-561b-403f-9b18-97eedbf7146b\n",
            "/tmp/ipython-input-694146001.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
            "/tmp/ipython-input-694146001.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-01 11:51:08,482] Trial 0 finished with value: 1.9839396160476062 and parameters: {'dropout': 0.44983997621252014, 'lr': 4.2286853291969677e-05, 'batch_size': 256, 'weight_decay': 2.701630494266463e-05, 'temperature': 0.03324723057875822, 'w_infonce': 0.7209312836976648, 'w_cos': 0.6142175661338521, 'w_mse': 0.45920387201388335, 'n_layers': 2, 'n_heads': 12, 'dim_feedforward': 1024}. Best is trial 0 with value: 1.9839396160476062.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE5CwJCFtzfB",
        "outputId": "3b76d196-1efd-43c0-e2ae-bdb55021e2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Parameters: 14,177,280\n",
            "\n",
            "3. Training...\n",
            "Computing Procrustes initialization...\n",
            "torch.Size([1536, 1024]) torch.Size([1536, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/45: 100%|██████████| 440/440 [00:08<00:00, 54.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 2.667712, Val Loss = 2.665481\n",
            "  ✓ Saved best model (val_loss=2.665481)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/45: 100%|██████████| 440/440 [00:08<00:00, 54.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 2.306717, Val Loss = 2.514931\n",
            "  ✓ Saved best model (val_loss=2.514931)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/45: 100%|██████████| 440/440 [00:08<00:00, 52.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 2.169662, Val Loss = 2.443939\n",
            "  ✓ Saved best model (val_loss=2.443939)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/45: 100%|██████████| 440/440 [00:08<00:00, 54.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 2.084842, Val Loss = 2.399220\n",
            "  ✓ Saved best model (val_loss=2.399220)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/45: 100%|██████████| 440/440 [00:08<00:00, 54.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 2.021432, Val Loss = 2.363772\n",
            "  ✓ Saved best model (val_loss=2.363772)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/45: 100%|██████████| 440/440 [00:07<00:00, 55.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss = 1.971583, Val Loss = 2.337160\n",
            "  ✓ Saved best model (val_loss=2.337160)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/45: 100%|██████████| 440/440 [00:08<00:00, 53.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss = 1.930606, Val Loss = 2.311470\n",
            "  ✓ Saved best model (val_loss=2.311470)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/45: 100%|██████████| 440/440 [00:08<00:00, 54.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss = 1.894185, Val Loss = 2.291556\n",
            "  ✓ Saved best model (val_loss=2.291556)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/45: 100%|██████████| 440/440 [00:07<00:00, 55.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss = 1.864162, Val Loss = 2.272109\n",
            "  ✓ Saved best model (val_loss=2.272109)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/45: 100%|██████████| 440/440 [00:08<00:00, 54.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss = 1.837341, Val Loss = 2.253808\n",
            "  ✓ Saved best model (val_loss=2.253808)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/45: 100%|██████████| 440/440 [00:08<00:00, 54.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss = 1.812634, Val Loss = 2.239229\n",
            "  ✓ Saved best model (val_loss=2.239229)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/45: 100%|██████████| 440/440 [00:08<00:00, 52.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Loss = 1.789839, Val Loss = 2.225551\n",
            "  ✓ Saved best model (val_loss=2.225551)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/45: 100%|██████████| 440/440 [00:07<00:00, 56.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Loss = 1.768866, Val Loss = 2.211253\n",
            "  ✓ Saved best model (val_loss=2.211253)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/45: 100%|██████████| 440/440 [00:08<00:00, 53.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss = 1.748797, Val Loss = 2.199894\n",
            "  ✓ Saved best model (val_loss=2.199894)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/45: 100%|██████████| 440/440 [00:08<00:00, 54.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Loss = 1.731735, Val Loss = 2.188339\n",
            "  ✓ Saved best model (val_loss=2.188339)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/45: 100%|██████████| 440/440 [00:08<00:00, 54.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss = 1.713526, Val Loss = 2.178325\n",
            "  ✓ Saved best model (val_loss=2.178325)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/45: 100%|██████████| 440/440 [00:08<00:00, 54.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Train Loss = 1.698392, Val Loss = 2.167685\n",
            "  ✓ Saved best model (val_loss=2.167685)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/45: 100%|██████████| 440/440 [00:07<00:00, 55.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Train Loss = 1.682995, Val Loss = 2.157980\n",
            "  ✓ Saved best model (val_loss=2.157980)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/45: 100%|██████████| 440/440 [00:07<00:00, 55.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Train Loss = 1.668335, Val Loss = 2.149243\n",
            "  ✓ Saved best model (val_loss=2.149243)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/45: 100%|██████████| 440/440 [00:08<00:00, 52.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss = 1.653344, Val Loss = 2.140630\n",
            "  ✓ Saved best model (val_loss=2.140630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/45: 100%|██████████| 440/440 [00:08<00:00, 54.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21: Train Loss = 1.639856, Val Loss = 2.133352\n",
            "  ✓ Saved best model (val_loss=2.133352)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/45: 100%|██████████| 440/440 [00:07<00:00, 55.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22: Train Loss = 1.626496, Val Loss = 2.124883\n",
            "  ✓ Saved best model (val_loss=2.124883)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/45: 100%|██████████| 440/440 [00:08<00:00, 53.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23: Train Loss = 1.615575, Val Loss = 2.117537\n",
            "  ✓ Saved best model (val_loss=2.117537)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/45: 100%|██████████| 440/440 [00:08<00:00, 53.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24: Train Loss = 1.602767, Val Loss = 2.109036\n",
            "  ✓ Saved best model (val_loss=2.109036)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/45: 100%|██████████| 440/440 [00:08<00:00, 54.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25: Train Loss = 1.589925, Val Loss = 2.102382\n",
            "  ✓ Saved best model (val_loss=2.102382)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/45: 100%|██████████| 440/440 [00:08<00:00, 54.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26: Train Loss = 1.579613, Val Loss = 2.096099\n",
            "  ✓ Saved best model (val_loss=2.096099)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/45: 100%|██████████| 440/440 [00:07<00:00, 55.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27: Train Loss = 1.568842, Val Loss = 2.089372\n",
            "  ✓ Saved best model (val_loss=2.089372)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/45: 100%|██████████| 440/440 [00:07<00:00, 56.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28: Train Loss = 1.557857, Val Loss = 2.082269\n",
            "  ✓ Saved best model (val_loss=2.082269)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/45: 100%|██████████| 440/440 [00:08<00:00, 52.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29: Train Loss = 1.547650, Val Loss = 2.077723\n",
            "  ✓ Saved best model (val_loss=2.077723)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/45: 100%|██████████| 440/440 [00:07<00:00, 55.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: Train Loss = 1.537787, Val Loss = 2.071424\n",
            "  ✓ Saved best model (val_loss=2.071424)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/45: 100%|██████████| 440/440 [00:07<00:00, 55.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31: Train Loss = 1.527890, Val Loss = 2.064726\n",
            "  ✓ Saved best model (val_loss=2.064726)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/45: 100%|██████████| 440/440 [00:08<00:00, 54.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32: Train Loss = 1.517575, Val Loss = 2.059332\n",
            "  ✓ Saved best model (val_loss=2.059332)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/45: 100%|██████████| 440/440 [00:09<00:00, 48.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33: Train Loss = 1.509626, Val Loss = 2.054208\n",
            "  ✓ Saved best model (val_loss=2.054208)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/45: 100%|██████████| 440/440 [00:07<00:00, 56.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34: Train Loss = 1.499840, Val Loss = 2.049011\n",
            "  ✓ Saved best model (val_loss=2.049011)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/45: 100%|██████████| 440/440 [00:08<00:00, 54.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35: Train Loss = 1.491968, Val Loss = 2.044377\n",
            "  ✓ Saved best model (val_loss=2.044377)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/45: 100%|██████████| 440/440 [00:08<00:00, 54.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36: Train Loss = 1.482622, Val Loss = 2.038578\n",
            "  ✓ Saved best model (val_loss=2.038578)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/45: 100%|██████████| 440/440 [00:08<00:00, 54.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37: Train Loss = 1.474448, Val Loss = 2.034065\n",
            "  ✓ Saved best model (val_loss=2.034065)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/45: 100%|██████████| 440/440 [00:08<00:00, 52.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38: Train Loss = 1.465391, Val Loss = 2.029970\n",
            "  ✓ Saved best model (val_loss=2.029970)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/45: 100%|██████████| 440/440 [00:08<00:00, 54.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39: Train Loss = 1.458482, Val Loss = 2.024600\n",
            "  ✓ Saved best model (val_loss=2.024600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/45: 100%|██████████| 440/440 [00:07<00:00, 55.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Train Loss = 1.450942, Val Loss = 2.020618\n",
            "  ✓ Saved best model (val_loss=2.020618)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/45: 100%|██████████| 440/440 [00:08<00:00, 53.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41: Train Loss = 1.443161, Val Loss = 2.016528\n",
            "  ✓ Saved best model (val_loss=2.016528)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/45: 100%|██████████| 440/440 [00:08<00:00, 52.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42: Train Loss = 1.435650, Val Loss = 2.011917\n",
            "  ✓ Saved best model (val_loss=2.011917)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/45: 100%|██████████| 440/440 [00:07<00:00, 56.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43: Train Loss = 1.428820, Val Loss = 2.008884\n",
            "  ✓ Saved best model (val_loss=2.008884)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/45: 100%|██████████| 440/440 [00:07<00:00, 55.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44: Train Loss = 1.421862, Val Loss = 2.005336\n",
            "  ✓ Saved best model (val_loss=2.005336)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/45: 100%|██████████| 440/440 [00:08<00:00, 54.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45: Train Loss = 1.413865, Val Loss = 2.001262\n",
            "  ✓ Saved best model (val_loss=2.001262)\n"
          ]
        }
      ],
      "source": [
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train\n",
        "print(\"\\n3. Training...\")\n",
        "model = training(model,\n",
        "                 train_loader,\n",
        "                 val_loader,\n",
        "                 DEVICE,\n",
        "                 EPOCHS,\n",
        "                 best_params[\"lr\"],\n",
        "                 MODEL_PATH,\n",
        "                 True,\n",
        "                 10000,\n",
        "                 best_params[\"temperature\"],\n",
        "                 best_params[\"w_infonce\"],\n",
        "                 best_params[\"w_mse\"],\n",
        "                 best_params[\"w_cos\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "metadata": {
        "id": "k_o7tFbwLHWX",
        "outputId": "538f3935-53e6-41e1-d9f0-d5c512cfd728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6uvjZJZt2PI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bafbb52a-b8c9-42a4-e392-ef69d9cd1547"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-651285137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_caption_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'captions/text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTRAIN_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_text_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg_VAL_SPLIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTRAIN_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_img_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images/names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_VAL_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mval_img_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images/embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_VAL_SPLIT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from challenge.src.eval import visualize_retrieval\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
        "val_text_embd = X_val\n",
        "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
        "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
        "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
        "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:,img_VAL_SPLIT])[1]\n",
        "\n",
        "# Sample and visualize\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(0, 100)\n",
        "    caption_embd = val_text_embd[idx]\n",
        "    caption_text = val_caption_text[idx]\n",
        "    gt_index = val_label[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_embds = model(caption_embd.to(DEVICE)).cpu()\n",
        "\n",
        "        visualize_retrieval(\n",
        "            pred_embds,\n",
        "            gt_index,\n",
        "            val_img_file,\n",
        "            caption_text, val_img_embd, k=5,\n",
        "            dataset_path = 'drive/MyDrive/data/train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RkbJt1-AuCGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd490f4-7e77-47fa-d110-2f4a8b8fd4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission file...\n",
            "✓ Saved submission to Transformer_submission.csv\n",
            "Model saved to: drive/MyDrive/data//models/transformer.pth\n"
          ]
        }
      ],
      "source": [
        "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
        "\n",
        "test_embds = test_data['captions/embeddings']\n",
        "test_embds = torch.from_numpy(test_embds).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
        "\n",
        "submission = generate_submission(test_data['captions/ids'], pred_embds, f'{choosen_arch}_submission.csv')\n",
        "print(f\"Model saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7EYrdanW7Fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}