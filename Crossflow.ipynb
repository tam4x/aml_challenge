{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb1136c",
   "metadata": {},
   "source": [
    "## Notebook for the Crossflow Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab770d75",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
    "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
    "#!unzip -o test.zip -d data\n",
    "#!unzip -o train.zip -d data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/Mamiglia/challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df810e23",
   "metadata": {},
   "source": [
    "### Create Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (B,) in [0,1]\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.exp(-torch.arange(half_dim, device=t.device) * (math.log(10000) / half_dim))\n",
    "        emb = t.unsqueeze(1) * emb.unsqueeze(0)  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)  # (B, dim)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFlowMLP(nn.Module):\n",
    "    def __init__(self, text_dim, img_dim, hidden_dim=1024, num_layers=3, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "        # Procrustes linear projection (initialized later)\n",
    "        self.proj = nn.Linear(text_dim, img_dim, bias=False)\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_emb = TimeEmbedding(time_emb_dim)\n",
    "        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        in_dim = img_dim  # after projection\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, img_dim))  # final output in image space\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t, z_text):\n",
    "        # 1. Project text embeddings into image space\n",
    "        z_proj = self.proj(z_text)\n",
    "\n",
    "        # 2. Compute time embedding\n",
    "        t_emb = self.time_emb(t)\n",
    "        t_emb = self.time_proj(t_emb)\n",
    "\n",
    "        # 3. Add residual time embedding\n",
    "        h = z_proj + t_emb  # simple residual addition\n",
    "\n",
    "        # 4. Pass through MLP\n",
    "        v = self.mlp(h)  # (B, img_dim)\n",
    "        return v, z_proj  # return velocity and projected text embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFlowTransformer(nn.Module):\n",
    "    def __init__(self, text_dim, img_dim, hidden_dim=512, num_layers=4, nhead=8, time_emb_dim=128, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Procrustes linear projection (initialized later)\n",
    "        self.proj = nn.Linear(text_dim, img_dim, bias=False)\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_emb = TimeEmbedding(time_emb_dim)\n",
    "        self.time_proj = nn.Linear(time_emb_dim, hidden_dim)\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(img_dim, hidden_dim)  # now in image space\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead,\n",
    "                                                   dim_feedforward=hidden_dim*4, activation='gelu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_dim, img_dim)\n",
    "\n",
    "    def forward(self, t, z_text):\n",
    "        \n",
    "        # Project text into image space\n",
    "        z_proj = self.proj(z_text)\n",
    "\n",
    "        # Time embedding\n",
    "        t_emb = self.time_emb(t)\n",
    "        t_emb = self.time_proj(t_emb)\n",
    "\n",
    "        # Input projection\n",
    "        h = self.input_proj(z_proj)\n",
    "\n",
    "        # Residual connection\n",
    "        h = h + t_emb\n",
    "\n",
    "        # Transformer\n",
    "        h = h.unsqueeze(0)\n",
    "        h = self.transformer(h)\n",
    "        h = h.squeeze(0)\n",
    "\n",
    "        # Output\n",
    "        v = self.output_proj(h) * self.temperature\n",
    "        return v, z_proj  # also return projected start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42f37c",
   "metadata": {},
   "source": [
    "### Flow Matching loss and inference euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c58b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_matching_loss(v_net, z_text, z_img):\n",
    "    B = z_text.size(0)\n",
    "    t = torch.rand(B, device=z_text.device)\n",
    "\n",
    "    # Forward pass\n",
    "    v_pred, z_proj = v_net(t, z_text)\n",
    "\n",
    "    # Linear path in image space\n",
    "    gamma = (1 - t).unsqueeze(1) * z_proj + t.unsqueeze(1) * z_img\n",
    "    gamma_dot = z_img - z_proj  # now shapes match\n",
    "\n",
    "    # Predicted velocity at gamma\n",
    "    v_pred_at_gamma = v_net(t, z_text)[0]  # same as v_pred\n",
    "    loss = nn.MSELoss()(v_pred_at_gamma, gamma_dot)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23258fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_euler(z_text, v_net, steps=50):\n",
    "    dt = 1.0 / steps\n",
    "    z = v_net.proj(z_text)  # start in image space\n",
    "    for i in range(steps):\n",
    "        t = torch.full((z.size(0),), i*dt, device=z.device)\n",
    "        dz, _ = v_net(t, z_text)\n",
    "        z = z + dz * dt\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52c8f9",
   "metadata": {},
   "source": [
    "### Procrustes Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f244d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Procrustes initialization ======\n",
    "def procrustes_init(text_embs, img_embs):\n",
    "    \"\"\"\n",
    "    text_embs: (N, d_text)\n",
    "    img_embs:  (N, d_img)\n",
    "    returns: weight matrix (d_img, d_text)\n",
    "    \"\"\"\n",
    "    # Center both\n",
    "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
    "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
    "\n",
    "    # Compute SVD of cross-covariance\n",
    "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
    "    W = U @ Vt  # orthogonal map d_text→d_img\n",
    "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
    "\n",
    "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
    "    \"\"\"Apply Procrustes initialization to the appropriate layer of the model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Compute Procrustes matrix\n",
    "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
    "\n",
    "        # Apply to the appropriate layer\n",
    "        applied = False\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(model, ResidualFlowTransformer) and name.endswith(\"proj\"):\n",
    "                print(m.weight.shape, W.shape)\n",
    "                if m.weight.shape == W.shape:\n",
    "                    m.weight.copy_(W)\n",
    "                    applied = True\n",
    "                    break\n",
    "        if not applied:\n",
    "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1104d",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1facf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crossflow\n",
    "# 4. Data Augmentation\n",
    "# 5. Zero Shot Stitching\n",
    "# 6. Diffusion Priors\n",
    "# Configuration\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load data\n",
    "train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "DATASET_SIZE = len(X)\n",
    "# Split train/val\n",
    "# This is done only to measure generalization capabilities, you don't have to\n",
    "# use a validation set (though we encourage this)\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891bae2",
   "metadata": {},
   "source": [
    "### Training and Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04755f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_crossflow(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
    "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07):\n",
    "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # --- Optional: Procrustes initialization ---\n",
    "    if use_procrustes_init:\n",
    "        print(\"Computing Procrustes initialization...\")\n",
    "        text_list, img_list = [], []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            text_list.append(X.cpu())\n",
    "            img_list.append(y.cpu())\n",
    "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
    "                break\n",
    "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
    "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
    "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
    "\n",
    "    # --- Training ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = flow_matching_loss(model, X_batch, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                loss = flow_matching_loss(model, X_batch, y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        # --- Save best model ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = ResidualFlowTransformer(text_dim=1024, img_dim=1536, time_emb_dim=BATCH_SIZE, temperature=0.2).to(DEVICE)\n",
    "model = LatentFlowMLP(text_dim=1024, img_dim=1536, time_emb_dim=BATCH_SIZE).to(DEVICE)\n",
    "MODEL_PATH = \"drive/MyDrive/data/models/crossflow.pth\"\n",
    "model = training_crossflow(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    DEVICE,\n",
    "    EPOCHS,\n",
    "    LR,\n",
    "    MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ed7c9",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ff0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
    "\n",
    "test_embds = test_data['captions/embeddings']\n",
    "test_embds = torch.from_numpy(test_embds).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_pred = integrate_euler(test_embds, model, steps=50)\n",
    "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
    "\n",
    "submission = generate_submission(test_data['captions/ids'], pred_embds, 'crossflow_submission.csv')\n",
    "MODEL_PATH = \"drive/MyDrive/data//models/crossflow.pth\"\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
