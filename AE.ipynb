{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab56e3b",
   "metadata": {},
   "source": [
    "#### File to test out Autoencoder and VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d038d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n",
    "#!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n",
    "#!unzip -o test.zip -d data\n",
    "#!unzip -o train.zip -d data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone https://github.com/Mamiglia/challenge.git\n",
    "!wget https://raw.githubusercontent.com/tam4x/aml_challenge/refs/heads/main/preprocess_data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9d7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75061d07",
   "metadata": {},
   "source": [
    "#### Create Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58623330",
   "metadata": {},
   "source": [
    "##### Normal Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09226d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TranslatorAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim, hidden_dim=512, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y_pred = self.decoder(z)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dad97",
   "metadata": {},
   "source": [
    "##### Autoencoder with Residual MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3875e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMLPHead(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class TranslatorRAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim, hidden_dim=512, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.residual_head = ResidualMLPHead(output_dim, hidden_dim=output_dim//2, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y_pred = self.decoder(z)\n",
    "        y_final = self.residual_head(y_pred)\n",
    "        return y_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca8294",
   "metadata": {},
   "source": [
    "##### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af35778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAETranslator(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim, hidden_dims=512, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dims))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dims\n",
    "        layers.append(nn.Linear(in_dim, latent_dim * 2))  # output μ and logσ\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # Decoder\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dims))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dims\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "        # Optional residual refinement\n",
    "        self.residual_head = ResidualMLPHead(output_dim, hidden_dim=output_dim//2, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input → (μ, logσ)\n",
    "        stats = self.encoder(x)\n",
    "        mu, log_sigma = stats.chunk(2, dim=-1)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = mu + sigma * eps\n",
    "\n",
    "        # Decode and refine\n",
    "        y_base = self.decoder(z)\n",
    "        y_final = self.residual_head(y_base)\n",
    "        return y_final, mu, log_sigma\n",
    "\n",
    "    def kl_loss(self, mu, log_sigma):\n",
    "        return -0.5 * torch.sum(1 + 2*log_sigma - mu.pow(2) - torch.exp(2*log_sigma), dim=-1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b35b8",
   "metadata": {},
   "source": [
    "### Training Loop and NCE Loss aswell as Procrustes Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8301770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QueueInfoNCELoss(nn.Module):\n",
    "    def __init__(self, dim, temperature=0.07, queue_size=4096):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.queue_size = queue_size\n",
    "        # queue shape: (queue_size, dim)\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, dim))\n",
    "        self.queue = F.normalize(self.queue, dim=1)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _enqueue(self, keys):\n",
    "        \"\"\"\n",
    "        keys: tensor (B, dim), already detached, normalized, on same device as queue.\n",
    "        This writes keys into the circular queue. Safe to call only AFTER backward.\n",
    "        \"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr.item())\n",
    "        end_ptr = (ptr + batch_size) % self.queue_size\n",
    "\n",
    "        if end_ptr > ptr:\n",
    "            self.queue[ptr:end_ptr] = keys\n",
    "        else:\n",
    "            # wrap\n",
    "            first_len = self.queue_size - ptr\n",
    "            self.queue[ptr:] = keys[:first_len]\n",
    "            self.queue[:end_ptr] = keys[first_len:]\n",
    "        self.queue_ptr[0] = end_ptr\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        Computes loss using current queue as negatives but does NOT modify the queue.\n",
    "        z_i: (B, dim) predicted (text -> img)\n",
    "        z_j: (B, dim) target (image)\n",
    "        \"\"\"\n",
    "        # normalize\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # positive logits: (B, 1)\n",
    "        l_pos = torch.sum(z_i * z_j, dim=-1, keepdim=True)\n",
    "\n",
    "        # negative logits from queue: (B, queue_size)\n",
    "        # queue is a buffer; safe to read\n",
    "        l_neg = torch.matmul(z_i, self.queue.T)\n",
    "\n",
    "        # logits: (B, 1 + queue_size)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        logits /= self.temperature\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=z_i.device)  # positives at index 0\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24c2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Procrustes initialization ======\n",
    "def procrustes_init(text_embs, img_embs):\n",
    "    \"\"\"\n",
    "    text_embs: (N, d_text)\n",
    "    img_embs:  (N, d_img)\n",
    "    returns: weight matrix (d_img, d_text)\n",
    "    \"\"\"\n",
    "    # Center both\n",
    "    X = text_embs - text_embs.mean(0, keepdim=True)\n",
    "    Y = img_embs - img_embs.mean(0, keepdim=True)\n",
    "\n",
    "    # Compute SVD of cross-covariance\n",
    "    U, _, Vt = torch.linalg.svd(X.T @ Y, full_matrices=False)\n",
    "    W = U @ Vt  # orthogonal map d_text→d_img\n",
    "    return W.T   # shape (d_img, d_text) for nn.Linear weight\n",
    "\n",
    "\n",
    "def apply_procrustes_init_to_final(model, text_sample, img_sample):\n",
    "    \"\"\"\n",
    "    Apply Procrustes initialization to a model.\n",
    "    - For MLP / ResidualMLP: apply to final Linear layer (hidden -> img_dim)\n",
    "    - For TransformerTranslator: apply to first projection (text_dim -> img_dim)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Compute Procrustes matrix\n",
    "        W = procrustes_init(text_embs=text_sample, img_embs=img_sample)\n",
    "\n",
    "        # Apply to the appropriate layer\n",
    "        applied = False\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Transformer: apply to first projection (proj_in)\n",
    "                if isinstance(model, TranslatorAE) and name.endswith(\"proj_in\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "                elif isinstance(model, TranslatorRAE) and name.endswith(\"proj_in\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "                elif isinstance(model, VAETranslator) and name.endswith(\"proj_in\"):\n",
    "                    print(m.weight.shape, W.shape)\n",
    "                    if m.weight.shape == W.shape:\n",
    "                        m.weight.copy_(W)\n",
    "                        applied = True\n",
    "                        break\n",
    "                    \n",
    "        if not applied:\n",
    "            print(\"⚠️ Warning: Could not find matching layer for Procrustes init\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5c9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training loop with Procrustes + InfoNCE ----------\n",
    "def training(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH,\n",
    "             use_procrustes_init=True, procrustes_subset=10000, temperature=0.07,\n",
    "             queue_size=4098):\n",
    "    \"\"\"Train LatentSpaceTranslator with optional Procrustes init + InfoNCE loss.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-3)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # --- Optional: Procrustes initialization ---\n",
    "    if use_procrustes_init:\n",
    "        print(\"Computing Procrustes initialization...\")\n",
    "        text_list, img_list = [], []\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            text_list.append(X.cpu())\n",
    "            img_list.append(y.cpu())\n",
    "            if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
    "                break\n",
    "        text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
    "        img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
    "        model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
    "\n",
    "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
    "    name = model.__class__.__name__\n",
    "    # --- Training ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = 0.0\n",
    "            if name == \"VAETranslator\":\n",
    "                pred, mu, log_sigma = model(X_batch)\n",
    "                kl_loss = model.kl_loss(mu, log_sigma)\n",
    "                loss += 1e-4 * kl_loss\n",
    "            else:\n",
    "                pred = model(X_batch)\n",
    "            # Weighted combination of losses\n",
    "            loss += 0.3 * criterion(pred, y_batch)\n",
    "            loss += 1 - F.cosine_similarity(pred, y_batch).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "              # put them into the queue\n",
    "              criterion._enqueue(keys)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                loss = 0.0\n",
    "                if name == \"VAETranslator\":\n",
    "                    pred, mu, log_sigma = model(X_batch)\n",
    "                    kl_loss = model.kl_loss(mu, log_sigma)\n",
    "                    loss += 1e-4 * kl_loss\n",
    "                else:\n",
    "                    pred = model(X_batch)\n",
    "                # Weighted combination of losses\n",
    "                loss += 0.3 * criterion(pred, y_batch)\n",
    "                loss += 1 - F.cosine_similarity(pred, y_batch).mean()\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "                criterion._enqueue(keys)\n",
    "\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        # --- Save best model ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab59241",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e32fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000,)\n",
      "Train data: 125000 captions, 125000 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([112500, 1536]), torch.Size([112500, 1024]), 512, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Data Augmentation\n",
    "# 5. Zero Shot Stitching\n",
    "# 6. Triplet Loss / Improve InfoNCE Loss / bidirectional / SimCLR / MoCo\n",
    "# 7. Autoencoder\n",
    "# Configuration\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 512\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load data\n",
    "#train_data = load_data(\"drive/MyDrive/data/train/train.npz\")\n",
    "train_data = load_data('data/train/train.npz')\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "DATASET_SIZE = len(X)\n",
    "# Split train/val\n",
    "# This is done only to measure generalization capabilities, you don't have to\n",
    "# use a validation set (though we encourage this)\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "y_train.shape, X_train.shape, train_loader.batch_size, val_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd8eb3",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb135a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (1.17.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\pier1\\anaconda3\\envs\\or\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE):\n",
    "\n",
    "    # --- Common hyperparameters ---\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "    #lr = trial.suggest_loguniform(\"lr\", 5e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
    "\n",
    "    # --- New hyperparameters ---\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.01, 0.2)\n",
    "    queue_size = trial.suggest_categorical(\"queue_size\", [2048, 4098, 8196])\n",
    "    #w_infonce = trial.suggest_float(\"w_infonce\", 0.6, 0.8)\n",
    "    #w_cos = trial.suggest_float(\"w_cos\", 0.4, 1.0)\n",
    "    #w_mse = trial.suggest_float(\"w_mse\", 1.0 - w_cos, 1.0)\n",
    "    procrustes_subset = 10000\n",
    "\n",
    "    # --- Architecture-specific hyperparameters ---\n",
    "    if arch in [\"AE\", \"RAE\", \"VAE\"]:\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [512, 1024, 2048])\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "        if arch == \"AE\":\n",
    "            latent_dim = trial.suggest_categorical(\"latent_dim\", [1024, 1536, 2048])\n",
    "            model = TranslatorAE(\n",
    "                input_dim=1024, latent_dim=latent_dim, output_dim=1536, hidden_dim=hidden_dim,\n",
    "                n_layers=num_layers, dropout=dropout\n",
    "            ).to(device)\n",
    "        elif arch == \"RAE\":\n",
    "            latent_dim = trial.suggest_categorical(\"latent_dim\", [1024, 1536, 2048])\n",
    "            model = TranslatorRAE(\n",
    "                input_dim=1024, latent_dim=latent_dim, output_dim=1536, hidden_dim=hidden_dim,\n",
    "                n_layers=num_layers, dropout=dropout\n",
    "            ).to(device)\n",
    "        else:\n",
    "            latent_dim = trial.suggest_categorical(\"latent_dim\", [512, 768, 1024])\n",
    "            model = VAETranslator(\n",
    "                input_dim=1024, latent_dim= latent_dim, output_dim=1536, hidden_dims=hidden_dim,\n",
    "                n_layers=num_layers, dropout=dropout\n",
    "            ).to(device)\n",
    "\n",
    "    # --- Apply Procrustes initialization ---\n",
    "    # if procrustes_subset > 0:\n",
    "    #     # Get subset from train_loader\n",
    "    #     text_list, img_list = [], []\n",
    "    #     for i, (X, y) in enumerate(train_loader):\n",
    "    #         text_list.append(X.cpu())\n",
    "    #         img_list.append(y.cpu())\n",
    "    #         if sum(t.shape[0] for t in text_list) >= procrustes_subset:\n",
    "    #             break\n",
    "    #     text_sample = torch.cat(text_list, dim=0)[:procrustes_subset]\n",
    "    #     img_sample = torch.cat(img_list, dim=0)[:procrustes_subset]\n",
    "    #     model = apply_procrustes_init_to_final(model, text_sample, img_sample)\n",
    "\n",
    "    criterion = QueueInfoNCELoss(dim=1536, temperature=temperature, queue_size=queue_size).to(device)\n",
    "    # --- Training loop (short run) ---\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(5):  # short training\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.0\n",
    "            if arch == \"VAE\":\n",
    "                pred, mu, log_sigma = model(X_batch)\n",
    "                kl_loss = model.kl_loss(mu, log_sigma)\n",
    "                loss += 1e-4 * kl_loss\n",
    "            else:\n",
    "                pred = model(X_batch)\n",
    "            # Weighted combination of losses\n",
    "            loss += 0.3 * criterion(pred, y_batch)\n",
    "            loss += 1 - F.cosine_similarity(pred, y_batch).mean()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "              keys = F.normalize(y_batch, dim=1).detach()   # image embeddings (targets) as keys\n",
    "              # put them into the queue\n",
    "              criterion._enqueue(keys)\n",
    "\n",
    "    # --- Evaluate on validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            loss = 0.0\n",
    "            if arch == \"VAE\":\n",
    "                pred, mu, log_sigma = model(X_batch)\n",
    "                kl_loss = model.kl_loss(mu, log_sigma)\n",
    "                loss += 1e-4 * kl_loss\n",
    "            else:\n",
    "                pred = model(X_batch)\n",
    "            # Weighted combination of losses\n",
    "            loss += 0.3 * criterion(pred, y_batch)\n",
    "            loss += 1 - F.cosine_similarity(pred, y_batch).mean()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            keys = F.normalize(y_batch, dim=1).detach()\n",
    "            criterion._enqueue(keys)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def run_optuna_extended(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=20):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
    "                   n_trials=n_trials)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Val loss: {trial.value}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc717d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 12:16:47,996] A new study created in memory with name: no-name-b1482e2e-09f0-4af3-ab35-aca001611f7a\n",
      "C:\\Users\\pier1\\AppData\\Local\\Temp\\ipykernel_7940\\521694892.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
      "[W 2025-11-12 12:17:33,013] Trial 0 failed with parameters: {'dropout': 0.364167064448793, 'weight_decay': 2.8136889906024196e-05, 'temperature': 0.14136867285894045, 'queue_size': 8196, 'hidden_dim': 1024, 'num_layers': 3, 'latent_dim': 512} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\pier1\\AppData\\Local\\Temp\\ipykernel_7940\\521694892.py\", line 115, in <lambda>\n",
      "    study.optimize(lambda trial: objective_extended(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE),\n",
      "                                 ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pier1\\AppData\\Local\\Temp\\ipykernel_7940\\521694892.py\", line 75, in objective_extended\n",
      "    loss.backward()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\_tensor.py\", line 647, in backward\n",
      "    torch.autograd.backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, gradient, retain_graph, create_graph, inputs=inputs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        tensors,\n",
      "        ^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        accumulate_grad=True,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 829, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        t_outputs, *args, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # Calls into the C++ engine to run the backward pass\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-12 12:17:33,015] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m archs = [\u001b[33m'\u001b[39m\u001b[33mAE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRAE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVAE\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m choosen_arch = archs[\u001b[32m2\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_params = \u001b[43mrun_optuna_extended\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43march\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoosen_arch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH_BASE\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/translator_optuna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mrun_optuna_extended\u001b[39m\u001b[34m(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_optuna_extended\u001b[39m(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=\u001b[32m20\u001b[39m):\n\u001b[32m    114\u001b[39m     study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH_BASE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m     trial = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mrun_optuna_extended.<locals>.<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_optuna_extended\u001b[39m(arch, train_dataloader, val_dataloader, device, MODEL_PATH_BASE, n_trials=\u001b[32m20\u001b[39m):\n\u001b[32m    114\u001b[39m     study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     study.optimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH_BASE\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    116\u001b[39m                    n_trials=n_trials)\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m     trial = study.best_trial\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mobjective_extended\u001b[39m\u001b[34m(arch, trial, train_dataloader, val_dataloader, device, MODEL_PATH_BASE)\u001b[39m\n\u001b[32m     71\u001b[39m loss += \u001b[32m0.3\u001b[39m * criterion(pred, y_batch)\n\u001b[32m     72\u001b[39m loss += \u001b[32m1\u001b[39m - F.cosine_similarity(pred, y_batch).mean()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     79\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pier1\\anaconda3\\envs\\OR\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "archs = ['AE', 'RAE', 'VAE']\n",
    "choosen_arch = archs[2]\n",
    "best_params = run_optuna_extended(\n",
    "    arch = choosen_arch,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    device=DEVICE,\n",
    "    MODEL_PATH_BASE=\"models/translator_optuna\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choosen_arch == 'AE':\n",
    "    model = TranslatorAE(\n",
    "        input_dim=1024,\n",
    "        latent_dim=best_params['latent_dim'],\n",
    "        output_dim=1536,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        n_layers=best_params['n_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data//models/AE.pth\"\n",
    "\n",
    "elif choosen_arch == 'RAE':\n",
    "    model = TranslatorRAE(\n",
    "    input_dim=1024,\n",
    "    latent_dim=best_params[\"latent_dim\"],\n",
    "    output_dim=1536,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    n_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data/models/RAE.pth\"\n",
    "\n",
    "else:\n",
    "    model = VAETranslator(\n",
    "    input_dim=1024,\n",
    "    latent_dim=best_params[\"latent_dim\"],\n",
    "    output_dim=1536,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    n_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]).to(DEVICE)\n",
    "    MODEL_PATH = \"drive/MyDrive/data/models/VAE.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n3. Training...\")\n",
    "model = training(model,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 DEVICE,\n",
    "                 EPOCHS,\n",
    "                 1e-6,\n",
    "                 MODEL_PATH,\n",
    "                 True,\n",
    "                 10000,\n",
    "                 best_params[\"temperature\"],\n",
    "                 best_params[\"queue_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14921c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "test_data = load_data(\"drive/MyDrive/data/test/test.clean.npz\")\n",
    "\n",
    "test_embds = test_data['captions/embeddings']\n",
    "test_embds = torch.from_numpy(test_embds).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
    "\n",
    "submission = generate_submission(test_data['captions/ids'], pred_embds, f'{choosen_arch}_submission.csv')\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
